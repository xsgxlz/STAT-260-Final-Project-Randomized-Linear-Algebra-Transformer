{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "seq_lengths = [32, 64, 128, 256]\n",
    "dims = [64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def rademacher_sketch(A, B, sketch_size):\n",
    "    S_shape = (*A.shape[:-2], A.shape[-1], sketch_size)\n",
    "    S = (torch.randint(0, 2, S_shape, device=A.device) * 2 - 1).float()\n",
    "    #S = torch.randn(S_shape, device=A.device)\n",
    "    AS = torch.matmul(A, S) / sketch_size\n",
    "    SB = torch.matmul(S.transpose(-1, -2), B)\n",
    "    AB_bar = torch.matmul(AS, SB)\n",
    "    print(S.shape, AS.shape, SB.shape, AB_bar.shape)\n",
    "    return AB_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Parameters\n",
    "batch_size = 8\n",
    "seq_lengths = [32, 64, 128, 256]\n",
    "dims = [512, 1024, 2048, 4096]\n",
    "sketch_sizes = [16, 32, 64]  # For RLA sketching\n",
    "num_trials = 10  # Number of runs for stable timing\n",
    "\n",
    "# Rademacher sketch function (from your code)\n",
    "def rademacher_sketch(A, B, sketch_size):\n",
    "    S_shape = (*A.shape[:-2], A.shape[-1], sketch_size)\n",
    "    S = (torch.randint(0, 2, S_shape, device=A.device) * 2 - 1).float()\n",
    "    AS = torch.matmul(A, S) / sketch_size  # Normalize for better approximation\n",
    "    SB = torch.matmul(S.transpose(-1, -2), B)\n",
    "    AB_bar = torch.matmul(AS, SB)\n",
    "    return AB_bar\n",
    "\n",
    "# Function to time matrix multiplication\n",
    "def time_mm(A, B, method='standard', sketch_size=None):\n",
    "    #torch.cuda.synchronize()  # Ensure GPU is ready\n",
    "    start = time.time()\n",
    "    \n",
    "    if method == 'standard':\n",
    "        result = torch.matmul(A, B)\n",
    "    elif method == 'rademacher':\n",
    "        result = rademacher_sketch(A, B, sketch_size)\n",
    "    \n",
    "    #torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "    end = time.time()\n",
    "    return end - start, result\n",
    "\n",
    "# Main experiment\n",
    "results = []\n",
    "for seq_len in seq_lengths:\n",
    "    for dim in dims:\n",
    "        # Input tensor (batch_size, seq_len, dim)\n",
    "        token = torch.randn(batch_size, seq_len, dim, device=device) + 2\n",
    "        \n",
    "        # Linear transformation to get Q and K (simulating Transformer projection)\n",
    "        linear_params = torch.randn(dim, dim, device=device) + 2\n",
    "        Q = torch.matmul(token, linear_params)  # [batch_size, seq_len, dim]\n",
    "        K = torch.matmul(token, linear_params)  # [batch_size, seq_len, dim]\n",
    "        K_T = K.transpose(-1, -2)  # [batch_size, dim, seq_len] for QK^T\n",
    "        \n",
    "        # Standard MM: Q K^T\n",
    "        standard_times = []\n",
    "        for _ in range(num_trials):\n",
    "            t, _ = time_mm(Q, K_T, method='standard')\n",
    "            standard_times.append(t)\n",
    "        standard_time = np.mean(standard_times)\n",
    "        \n",
    "        # RLA MM for different sketch sizes\n",
    "        rla_times = {}\n",
    "        for sketch_size in sketch_sizes:\n",
    "            rla_times_trial = []\n",
    "            for _ in range(num_trials):\n",
    "                t, _ = time_mm(Q, K_T, method='rademacher', sketch_size=sketch_size)\n",
    "                rla_times_trial.append(t)\n",
    "            rla_times[sketch_size] = np.mean(rla_times_trial)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'seq_len': seq_len,\n",
    "            'dim': dim,\n",
    "            'standard_time': standard_time,\n",
    "            'rla_times': rla_times,\n",
    "            'speedups': {k: standard_time / rla_times[k] for k in rla_times}\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nTiming Results (seconds) and Speedup (Standard / RLA):\")\n",
    "print(f\"{'Seq Len':>8} {'Dim':>6} {'Standard':>10} {'RLA k=16':>10} {'Speedup':>8} {'RLA k=32':>10} {'Speedup':>8} {'RLA k=64':>10} {'Speedup':>8}\")\n",
    "for res in results:\n",
    "    print(f\"{res['seq_len']:>8} {res['dim']:>6} {res['standard_time']:>10.6f} \"\n",
    "          f\"{res['rla_times'][16]:>10.6f} {res['speedups'][16]:>8.3f} \"\n",
    "          f\"{res['rla_times'][32]:>10.6f} {res['speedups'][32]:>8.3f} \"\n",
    "          f\"{res['rla_times'][64]:>10.6f} {res['speedups'][64]:>8.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
