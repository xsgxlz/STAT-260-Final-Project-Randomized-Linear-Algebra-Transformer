{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time stamp: 2025-04-01 18-31-36\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"/jet/home/azhang19/stat 260/STAT-260-Final-Project-Randomized-Linear-Algebra-Transformer\")\n",
    "from RLALLaMA3.LLaMA3 import ModelArgs\n",
    "from RLALLaMA3.utils import (\n",
    "    linear_warmup_cosine_decay_multiplicative,\n",
    "    name_args,\n",
    "    transformer_forward_pass,\n",
    "    Args,\n",
    ")\n",
    "from RLALLaMA3.tasks import single_answer_seq_loss, get_dataset\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#import torch._dynamo\n",
    "#torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "time_stamp = time.strftime(\"%Y-%m-%d %H-%M-%S\", time.localtime())\n",
    "print(f\"Time stamp: {time_stamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args Configuration:\n",
      "\n",
      "Training Parameters:\n",
      "  standard_lr:        3.2e-03\n",
      "  standard_epoch:     80000\n",
      "  standard_warmup_steps: 4000\n",
      "  batch_size:         768\n",
      "  min_lr:             1.0e-04\n",
      "  grad_clip_max_norm: 1.0\n",
      "  use_amp:            True\n",
      "  use_compile:       False\n",
      "\n",
      "Data Parameters:\n",
      "  task:              number_add\n",
      "  max_level:         20\n",
      "  random_seq_len:    True\n",
      "  number_range:      (0, 99)\n",
      "\n",
      "Model Architecture Parameters:\n",
      "  dim:               32\n",
      "  n_layers:          2\n",
      "  n_heads:           4\n",
      "  hidden_dim:        112\n",
      "\n",
      "Save Path Parameters:\n",
      "  save_path:         /accounts/grad/zhangyunzhe2023/Neural-ODE/ckpt\n",
      "  final_save_path:   /accounts/grad/zhangyunzhe2023/Neural-ODE/ckpt_final\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the arguments\n",
    "##args = parse_args()\n",
    "args = Args(\n",
    "    # Training\n",
    "    standard_lr=3.16e-3,\n",
    "    standard_epoch=80000,\n",
    "    standard_warmup_steps=4000,\n",
    "    batch_size=768,\n",
    "    min_lr=1e-4,\n",
    "    grad_clip_max_norm=1.0,\n",
    "    use_amp=True,\n",
    "    use_compile=False,\n",
    "\n",
    "    # Data\n",
    "    task=\"number_add\",\n",
    "    max_level=20,\n",
    "    random_seq_len=True,\n",
    "    number_range=(0, 99),\n",
    "\n",
    "    # Model\n",
    "    dim=32,\n",
    "    n_layers=2,\n",
    "    n_heads=4,\n",
    "    hidden_dim=112,\n",
    "\n",
    "    # Save\n",
    "    save_path=\"/accounts/grad/zhangyunzhe2023/Neural-ODE/ckpt\",\n",
    "    final_save_path=\"/accounts/grad/zhangyunzhe2023/Neural-ODE/ckpt_final\",\n",
    ")\n",
    "\n",
    "print(args, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "dataset, collate_fn, vocab_size, max_seq_len = get_dataset(args.task,\n",
    "                                                           args.max_level,\n",
    "                                                           args.random_seq_len,\n",
    "                                                           args.number_range,\n",
    "                                                           nested_tensor=False,\n",
    "                                                           pad_to_longest=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, collate_fn=collate_fn,\n",
    "                                         num_workers=torch.get_num_threads(), pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sequence length: 34.37289093017578\n",
      "Max sequence length: 66\n"
     ]
    }
   ],
   "source": [
    "def mean_seq_len(dataloader, num_samples=100):\n",
    "    \"\"\"\n",
    "    Calculate the mean sequence length of the dataset.\n",
    "    \"\"\"\n",
    "    total_len = 0\n",
    "    num_samples = min(num_samples, len(dataloader.dataset))\n",
    "    for i, x in enumerate(dataloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        total_len += x[1].float().mean().item()\n",
    "    return total_len / num_samples\n",
    "\n",
    "mean_len = mean_seq_len(dataloader)\n",
    "print(f\"Mean sequence length: {mean_len}\")\n",
    "print(f\"Max sequence length: {max_seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model\n",
    "\n",
    "transformer_args = ModelArgs(\n",
    "    dim=args.dim,\n",
    "    n_layers=args.n_layers,\n",
    "    n_heads=args.n_heads,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    norm_eps=1e-5,\n",
    "    rope_theta=500000,\n",
    "    max_seq_len=max_seq_len,\n",
    "\n",
    "    sketch_mode = 'rademacher',\n",
    "    attention_qkv_sketch_size = 16,\n",
    "    attention_out_sketch_size = 16,\n",
    "    feedforward_sketch_size_in = 16,\n",
    "    feedforward_sketch_size_out = 64,\n",
    "    deterministic = False\n",
    ")\n",
    "\n",
    "from RLALLaMA3.LLaMA3 import Transformer\n",
    "model = Transformer(params=transformer_args)\n",
    "\n",
    "model = model.to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived Parameters:\n",
      "lr: 0.00474\n",
      "warmup_steps: 2666\n",
      "epochs: 53333\n",
      "grad_clip_max_norm: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "standard_lr = args.standard_lr / 512\n",
    "standard_epoch = args.standard_epoch * 512\n",
    "standard_warmup_steps = args.standard_warmup_steps * 512\n",
    "batch_size = args.batch_size\n",
    "\n",
    "lr = standard_lr * batch_size\n",
    "warmup_steps = standard_warmup_steps // batch_size\n",
    "epochs = standard_epoch // batch_size\n",
    "\n",
    "print(\"Derived Parameters:\")\n",
    "print(f\"lr: {lr}\")\n",
    "print(f\"warmup_steps: {warmup_steps}\")\n",
    "print(f\"epochs: {epochs}\")\n",
    "print(f\"grad_clip_max_norm: {args.grad_clip_max_norm}\", end=\"\\n\\n\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, fused=True)\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer,\n",
    "            lr_lambda=lambda step: linear_warmup_cosine_decay_multiplicative(step, warmup_steps, epochs, args.min_lr))\n",
    "\n",
    "scaler = torch.amp.GradScaler(device, enabled=args.use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and arguments\n",
    "\n",
    "def save_record(path, model, record, args, time_stamp, extra_info=None):\n",
    "    dict_name, file_name = name_args(args, \"_\")\n",
    "\n",
    "    os.makedirs(f\"{path}/{dict_name}\", exist_ok=True)\n",
    "    file_name = file_name + f\"_{time_stamp}\"\n",
    "    if extra_info is not None:\n",
    "        file_name += f\"_{extra_info}\"\n",
    "    \n",
    "    record_dict = {\n",
    "        \"model\": model,\n",
    "        \"record\": record,\n",
    "        \"args\": args,\n",
    "        \"time_stamp\": time_stamp,\n",
    "    }\n",
    "        \n",
    "    torch.save(record_dict, f\"{path}/{dict_name}/{file_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backwards pass\n",
    "def backward_pass(model, loss, optimizer, scaler, scheduler, grad_clip_max_norm):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(disable=not args.use_compile)\n",
    "def train_step(model, train_data, mean_len, optimizer, scheduler, scaler, args):\n",
    "    device = train_data[0].device\n",
    "    \n",
    "    with torch.autocast(device_type=device.type, dtype=torch.bfloat16, enabled=args.use_amp):\n",
    "        tokens, lengths, ans_starts, ans_lengths = train_data\n",
    "        pred = transformer_forward_pass(tokens[:, :-1], model)\n",
    "        \n",
    "        result = single_answer_seq_loss(pred, tokens, lengths, ans_starts, ans_lengths)\n",
    "        GPT_loss, full_seq_acc, ans_region_acc, ans_char_acc = result\n",
    "        # Normalize the GPT loss by the batch size but not the sequence length\n",
    "        GPT_loss = GPT_loss / args.batch_size\n",
    "        total_loss = GPT_loss\n",
    "        total_loss_for_backward = total_loss / mean_len\n",
    "    \n",
    "    if torch.isnan(total_loss) or torch.isinf(total_loss):# or (total_loss > smoothed_loss * 1.1):\n",
    "        return [total_loss]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        safe_params = [copy.deepcopy(i.state_dict()) for i in [model, optimizer, scheduler]]\n",
    "\n",
    "    backward_pass(model, total_loss_for_backward, optimizer, scaler, scheduler, args.grad_clip_max_norm)\n",
    "    \n",
    "    data = [GPT_loss, 0, total_loss, full_seq_acc, ans_region_acc, ans_char_acc]\n",
    "    \n",
    "    data += [0, 0]\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        data = torch.tensor(data).cpu().numpy()\n",
    "\n",
    "    return data, safe_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "GPT loss: 94.88225555419922\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.88225555419922\n",
      "Full Seq Acc: 0.07002757489681244\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0813223123550415\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.88225555419922\n",
      "Time: 0.4295940399169922\n",
      "\n",
      "Epoch: 2\n",
      "GPT loss: 95.43161010742188\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 95.43161010742188\n",
      "Full Seq Acc: 0.11606083065271378\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0723208412528038\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.88774909973144\n",
      "Time: 0.07771754264831543\n",
      "\n",
      "Epoch: 3\n",
      "GPT loss: 95.12255859375\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 95.12255859375\n",
      "Full Seq Acc: 0.11765364557504654\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06352324783802032\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.89009719467161\n",
      "Time: 0.048958539962768555\n",
      "\n",
      "Epoch: 4\n",
      "GPT loss: 95.12603759765625\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 95.12603759765625\n",
      "Full Seq Acc: 0.12103992700576782\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.07344262301921844\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.89245659870146\n",
      "Time: 0.048964500427246094\n",
      "\n",
      "Epoch: 5\n",
      "GPT loss: 92.8864974975586\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.8864974975586\n",
      "Full Seq Acc: 0.11751486361026764\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.058009225875139236\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.87239700769003\n",
      "Time: 0.04720258712768555\n",
      "\n",
      "Epoch: 6\n",
      "GPT loss: 94.49434661865234\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.49434661865234\n",
      "Full Seq Acc: 0.12045037746429443\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06223246455192566\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.86861650379966\n",
      "Time: 0.045549869537353516\n",
      "\n",
      "Epoch: 7\n",
      "GPT loss: 93.35212707519531\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.35212707519531\n",
      "Full Seq Acc: 0.11726804077625275\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06108374148607254\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.85345160951361\n",
      "Time: 0.04553699493408203\n",
      "\n",
      "Epoch: 8\n",
      "GPT loss: 96.18456268310547\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 96.18456268310547\n",
      "Full Seq Acc: 0.12027176469564438\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.058862216770648956\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.86676272024953\n",
      "Time: 0.04553842544555664\n",
      "\n",
      "Epoch: 9\n",
      "GPT loss: 91.497314453125\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.497314453125\n",
      "Full Seq Acc: 0.12554563581943512\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05971626192331314\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.83306823757827\n",
      "Time: 0.043813467025756836\n",
      "\n",
      "Epoch: 10\n",
      "GPT loss: 96.29549407958984\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 96.29549407958984\n",
      "Full Seq Acc: 0.12150321155786514\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06712508201599121\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.84769249599839\n",
      "Time: 0.04368162155151367\n",
      "\n",
      "Epoch: 11\n",
      "GPT loss: 92.03636932373047\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.03636932373047\n",
      "Full Seq Acc: 0.11903481930494308\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.07049180567264557\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.8195792642757\n",
      "Time: 0.043649911880493164\n",
      "\n",
      "Epoch: 12\n",
      "GPT loss: 96.47283935546875\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 96.47283935546875\n",
      "Full Seq Acc: 0.12070102989673615\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06276560574769974\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.83611186518763\n",
      "Time: 0.04393911361694336\n",
      "\n",
      "Epoch: 13\n",
      "GPT loss: 97.18385314941406\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 97.18385314941406\n",
      "Full Seq Acc: 0.11925005167722702\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0601973719894886\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.8595892780299\n",
      "Time: 0.04407310485839844\n",
      "\n",
      "Epoch: 14\n",
      "GPT loss: 95.70237731933594\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 95.70237731933594\n",
      "Full Seq Acc: 0.12241964042186737\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.061003610491752625\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.86801715844295\n",
      "Time: 0.044211387634277344\n",
      "\n",
      "Epoch: 15\n",
      "GPT loss: 92.34896850585938\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.34896850585938\n",
      "Full Seq Acc: 0.11864017695188522\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05965721607208252\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.84282667191712\n",
      "Time: 0.043967485427856445\n",
      "\n",
      "Epoch: 16\n",
      "GPT loss: 93.50914764404297\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.50914764404297\n",
      "Full Seq Acc: 0.12436289340257645\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05891980230808258\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.82948988163838\n",
      "Time: 0.04445362091064453\n",
      "\n",
      "Epoch: 17\n",
      "GPT loss: 91.301513671875\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.301513671875\n",
      "Full Seq Acc: 0.12513034045696259\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06929392367601395\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.79421011954075\n",
      "Time: 0.04432368278503418\n",
      "\n",
      "Epoch: 18\n",
      "GPT loss: 92.25730895996094\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.25730895996094\n",
      "Full Seq Acc: 0.12417750805616379\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.058552633970975876\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.76884110794495\n",
      "Time: 0.04434990882873535\n",
      "\n",
      "Epoch: 19\n",
      "GPT loss: 91.67382049560547\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.67382049560547\n",
      "Full Seq Acc: 0.12428747862577438\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06208718940615654\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.73789090182156\n",
      "Time: 0.044309377670288086\n",
      "\n",
      "Epoch: 20\n",
      "GPT loss: 94.5036849975586\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.5036849975586\n",
      "Full Seq Acc: 0.12668554484844208\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06157877296209335\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.73554884277893\n",
      "Time: 0.04432415962219238\n",
      "\n",
      "Epoch: 21\n",
      "GPT loss: 91.73297119140625\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.73297119140625\n",
      "Full Seq Acc: 0.12309170514345169\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06113681569695473\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.7055230662652\n",
      "Time: 0.0441441535949707\n",
      "\n",
      "Epoch: 22\n",
      "GPT loss: 93.51821899414062\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.51821899414062\n",
      "Full Seq Acc: 0.1302042156457901\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06471747905015945\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.69365002554395\n",
      "Time: 0.043999433517456055\n",
      "\n",
      "Epoch: 23\n",
      "GPT loss: 95.14012145996094\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 95.14012145996094\n",
      "Full Seq Acc: 0.12564359605312347\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.066470205783844\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.69811473988811\n",
      "Time: 0.04408407211303711\n",
      "\n",
      "Epoch: 24\n",
      "GPT loss: 94.24849700927734\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.24849700927734\n",
      "Full Seq Acc: 0.12736853957176208\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.061821769922971725\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.693618562582\n",
      "Time: 0.0438838005065918\n",
      "\n",
      "Epoch: 25\n",
      "GPT loss: 92.6388168334961\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.6388168334961\n",
      "Full Seq Acc: 0.1269817054271698\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06495219469070435\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.67307054529115\n",
      "Time: 0.04424548149108887\n",
      "\n",
      "Epoch: 26\n",
      "GPT loss: 93.2806625366211\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.2806625366211\n",
      "Full Seq Acc: 0.1271381676197052\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.057114556431770325\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.65914646520446\n",
      "Time: 0.04400920867919922\n",
      "\n",
      "Epoch: 27\n",
      "GPT loss: 92.45669555664062\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.45669555664062\n",
      "Full Seq Acc: 0.12827932834625244\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0597359761595726\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.63712195611882\n",
      "Time: 0.04400753974914551\n",
      "\n",
      "Epoch: 28\n",
      "GPT loss: 93.50743103027344\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.50743103027344\n",
      "Full Seq Acc: 0.13319294154644012\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06434668600559235\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.62582504686037\n",
      "Time: 0.04364609718322754\n",
      "\n",
      "Epoch: 29\n",
      "GPT loss: 94.42396545410156\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.42396545410156\n",
      "Full Seq Acc: 0.12827585637569427\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0621931254863739\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.62380645093279\n",
      "Time: 0.04372000694274902\n",
      "\n",
      "Epoch: 30\n",
      "GPT loss: 91.0188217163086\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.0188217163086\n",
      "Full Seq Acc: 0.13627779483795166\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06299472600221634\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.58775660358654\n",
      "Time: 0.04377484321594238\n",
      "\n",
      "Epoch: 31\n",
      "GPT loss: 91.39803314208984\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.39803314208984\n",
      "Full Seq Acc: 0.13330769538879395\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.060376111418008804\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.55585936897158\n",
      "Time: 0.04362964630126953\n",
      "\n",
      "Epoch: 32\n",
      "GPT loss: 94.5334701538086\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.5334701538086\n",
      "Full Seq Acc: 0.13496273756027222\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05917159840464592\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.55563547681996\n",
      "Time: 0.04351544380187988\n",
      "\n",
      "Epoch: 33\n",
      "GPT loss: 91.13458251953125\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.13458251953125\n",
      "Full Seq Acc: 0.13455386459827423\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06417465955018997\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.52142494724707\n",
      "Time: 0.043604373931884766\n",
      "\n",
      "Epoch: 34\n",
      "GPT loss: 92.49446868896484\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.49446868896484\n",
      "Full Seq Acc: 0.13611532747745514\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05646749958395958\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.50115538466424\n",
      "Time: 0.043686628341674805\n",
      "\n",
      "Epoch: 35\n",
      "GPT loss: 93.50798797607422\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.50798797607422\n",
      "Full Seq Acc: 0.1388888955116272\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06932980567216873\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.49122371057834\n",
      "Time: 0.04399585723876953\n",
      "\n",
      "Epoch: 36\n",
      "GPT loss: 93.74151611328125\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.74151611328125\n",
      "Full Seq Acc: 0.13684958219528198\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06332021206617355\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.48372663460538\n",
      "Time: 0.04369330406188965\n",
      "\n",
      "Epoch: 37\n",
      "GPT loss: 94.41844177246094\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.41844177246094\n",
      "Full Seq Acc: 0.14302648603916168\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05378812924027443\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.48307378598393\n",
      "Time: 0.043534040451049805\n",
      "\n",
      "Epoch: 38\n",
      "GPT loss: 94.49840545654297\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.49840545654297\n",
      "Full Seq Acc: 0.13967536389827728\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05424836650490761\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.48322710268951\n",
      "Time: 0.04366874694824219\n",
      "\n",
      "Epoch: 39\n",
      "GPT loss: 94.17449951171875\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.17449951171875\n",
      "Full Seq Acc: 0.1460040807723999\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05767963081598282\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.4801398267798\n",
      "Time: 0.0435030460357666\n",
      "\n",
      "Epoch: 40\n",
      "GPT loss: 92.55900573730469\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.55900573730469\n",
      "Full Seq Acc: 0.14418429136276245\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.055628702044487\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.46092848588505\n",
      "Time: 0.04387831687927246\n",
      "\n",
      "Epoch: 41\n",
      "GPT loss: 91.94853210449219\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.94853210449219\n",
      "Full Seq Acc: 0.1418585181236267\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.052562415599823\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.43580452207111\n",
      "Time: 0.043704986572265625\n",
      "\n",
      "Epoch: 42\n",
      "GPT loss: 92.11199188232422\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.11199188232422\n",
      "Full Seq Acc: 0.14825758337974548\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.06506737321615219\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.41256639567364\n",
      "Time: 0.043642282485961914\n",
      "\n",
      "Epoch: 43\n",
      "GPT loss: 91.94322967529297\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.94322967529297\n",
      "Full Seq Acc: 0.15194667875766754\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.052251070737838745\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.38787302846983\n",
      "Time: 0.04360699653625488\n",
      "\n",
      "Epoch: 44\n",
      "GPT loss: 92.85523986816406\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.85523986816406\n",
      "Full Seq Acc: 0.153681680560112\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05506391450762749\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.37254669686678\n",
      "Time: 0.0434722900390625\n",
      "\n",
      "Epoch: 45\n",
      "GPT loss: 90.78419494628906\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.78419494628906\n",
      "Full Seq Acc: 0.14926347136497498\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0520421601831913\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.336663179361\n",
      "Time: 0.043546199798583984\n",
      "\n",
      "Epoch: 46\n",
      "GPT loss: 91.05376434326172\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.05376434326172\n",
      "Full Seq Acc: 0.14995786547660828\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0635286346077919\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.303834191\n",
      "Time: 0.04380011558532715\n",
      "\n",
      "Epoch: 47\n",
      "GPT loss: 93.2208251953125\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.2208251953125\n",
      "Full Seq Acc: 0.151775062084198\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05334211140871048\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.29300410104314\n",
      "Time: 0.04383039474487305\n",
      "\n",
      "Epoch: 48\n",
      "GPT loss: 90.21038055419922\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.21038055419922\n",
      "Full Seq Acc: 0.15569879114627838\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05676567927002907\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.25217786557471\n",
      "Time: 0.0434873104095459\n",
      "\n",
      "Epoch: 49\n",
      "GPT loss: 90.0655746459961\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.0655746459961\n",
      "Full Seq Acc: 0.15052391588687897\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05641702190041542\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.21031183337892\n",
      "Time: 0.043492794036865234\n",
      "\n",
      "Epoch: 50\n",
      "GPT loss: 90.32115173339844\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.32115173339844\n",
      "Full Seq Acc: 0.15485310554504395\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.054054051637649536\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.17142023237912\n",
      "Time: 0.04334616661071777\n",
      "\n",
      "Epoch: 51\n",
      "GPT loss: 92.03717041015625\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.03717041015625\n",
      "Full Seq Acc: 0.1611335128545761\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.055849310010671616\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.1500777341569\n",
      "Time: 0.0433804988861084\n",
      "\n",
      "Epoch: 52\n",
      "GPT loss: 91.88716888427734\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.88716888427734\n",
      "Full Seq Acc: 0.15694659948349\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05493421107530594\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.12744864565809\n",
      "Time: 0.043352603912353516\n",
      "\n",
      "Epoch: 53\n",
      "GPT loss: 91.69572448730469\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.69572448730469\n",
      "Full Seq Acc: 0.16579900681972504\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05088640749454498\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.10313140407456\n",
      "Time: 0.04334545135498047\n",
      "\n",
      "Epoch: 54\n",
      "GPT loss: 88.6907958984375\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.6907958984375\n",
      "Full Seq Acc: 0.16908608376979828\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05154300853610039\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.0490080490182\n",
      "Time: 0.04332304000854492\n",
      "\n",
      "Epoch: 55\n",
      "GPT loss: 94.16735076904297\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.16735076904297\n",
      "Full Seq Acc: 0.16598105430603027\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05663483589887619\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.05019147621844\n",
      "Time: 0.04356050491333008\n",
      "\n",
      "Epoch: 56\n",
      "GPT loss: 92.21839904785156\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.21839904785156\n",
      "Full Seq Acc: 0.16325993835926056\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0482441745698452\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 94.03187355193478\n",
      "Time: 0.043943166732788086\n",
      "\n",
      "Epoch: 57\n",
      "GPT loss: 90.51614379882812\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.51614379882812\n",
      "Full Seq Acc: 0.16928833723068237\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05432992801070213\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.99671625440371\n",
      "Time: 0.043805837631225586\n",
      "\n",
      "Epoch: 58\n",
      "GPT loss: 93.09379577636719\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.09379577636719\n",
      "Full Seq Acc: 0.16749629378318787\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04407443851232529\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.98768704962335\n",
      "Time: 0.04387974739074707\n",
      "\n",
      "Epoch: 59\n",
      "GPT loss: 90.30522918701172\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.30522918701172\n",
      "Full Seq Acc: 0.16942796111106873\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05186653137207031\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.95086247099722\n",
      "Time: 0.043732404708862305\n",
      "\n",
      "Epoch: 60\n",
      "GPT loss: 91.36639404296875\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.36639404296875\n",
      "Full Seq Acc: 0.18047180771827698\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.051087670028209686\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.92501778671694\n",
      "Time: 0.04350686073303223\n",
      "\n",
      "Epoch: 61\n",
      "GPT loss: 91.15263366699219\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.15263366699219\n",
      "Full Seq Acc: 0.17709238827228546\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04702400788664818\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.89729394551969\n",
      "Time: 0.043479204177856445\n",
      "\n",
      "Epoch: 62\n",
      "GPT loss: 92.49594116210938\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.49594116210938\n",
      "Full Seq Acc: 0.17608332633972168\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04478103294968605\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.88328041768558\n",
      "Time: 0.0436556339263916\n",
      "\n",
      "Epoch: 63\n",
      "GPT loss: 91.21144104003906\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.21144104003906\n",
      "Full Seq Acc: 0.18443022668361664\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05245901644229889\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.85656202390912\n",
      "Time: 0.04391145706176758\n",
      "\n",
      "Epoch: 64\n",
      "GPT loss: 92.02855682373047\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.02855682373047\n",
      "Full Seq Acc: 0.17908886075019836\n",
      "Ans Region Acc: 0.0013020833721384406\n",
      "Ans Char Acc: 0.04766600951552391\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.83828197190734\n",
      "Time: 0.04370760917663574\n",
      "\n",
      "Epoch: 65\n",
      "GPT loss: 90.69841766357422\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.69841766357422\n",
      "Full Seq Acc: 0.18719042837619781\n",
      "Ans Region Acc: 0.0013020833721384406\n",
      "Ans Char Acc: 0.05190538614988327\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.806883328824\n",
      "Time: 0.04361104965209961\n",
      "\n",
      "Epoch: 66\n",
      "GPT loss: 89.62627410888672\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.62627410888672\n",
      "Full Seq Acc: 0.1852107048034668\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04542462155222893\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.76507723662462\n",
      "Time: 0.04364633560180664\n",
      "\n",
      "Epoch: 67\n",
      "GPT loss: 94.61395263671875\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.61395263671875\n",
      "Full Seq Acc: 0.1908223032951355\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04326450452208519\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.77356599062557\n",
      "Time: 0.04350876808166504\n",
      "\n",
      "Epoch: 68\n",
      "GPT loss: 92.73503875732422\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.73503875732422\n",
      "Full Seq Acc: 0.19127194583415985\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05174686759710312\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.76318071829256\n",
      "Time: 0.04376411437988281\n",
      "\n",
      "Epoch: 69\n",
      "GPT loss: 91.81784057617188\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.81784057617188\n",
      "Full Seq Acc: 0.1965208351612091\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.043435342609882355\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.74372731687136\n",
      "Time: 0.043749094009399414\n",
      "\n",
      "Epoch: 70\n",
      "GPT loss: 90.2577133178711\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.2577133178711\n",
      "Full Seq Acc: 0.19620229303836823\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04996712878346443\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.70886717688136\n",
      "Time: 0.043762922286987305\n",
      "\n",
      "Epoch: 71\n",
      "GPT loss: 91.66181945800781\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.66181945800781\n",
      "Full Seq Acc: 0.20239952206611633\n",
      "Ans Region Acc: 0.0013020833721384406\n",
      "Ans Char Acc: 0.04006567969918251\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.68839669969262\n",
      "Time: 0.04367494583129883\n",
      "\n",
      "Epoch: 72\n",
      "GPT loss: 88.42931365966797\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.42931365966797\n",
      "Full Seq Acc: 0.1980331689119339\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03999999910593033\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.63580586929237\n",
      "Time: 0.04340171813964844\n",
      "\n",
      "Epoch: 73\n",
      "GPT loss: 88.10665130615234\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.10665130615234\n",
      "Full Seq Acc: 0.20214945077896118\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04018445312976837\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.58051432366098\n",
      "Time: 0.04348611831665039\n",
      "\n",
      "Epoch: 74\n",
      "GPT loss: 90.28353881835938\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.28353881835938\n",
      "Full Seq Acc: 0.21022792160511017\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04279131069779396\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.54754456860796\n",
      "Time: 0.04376983642578125\n",
      "\n",
      "Epoch: 75\n",
      "GPT loss: 88.6469955444336\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.6469955444336\n",
      "Full Seq Acc: 0.2102240025997162\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04791804403066635\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.49853907836622\n",
      "Time: 0.0440371036529541\n",
      "\n",
      "Epoch: 76\n",
      "GPT loss: 89.10863494873047\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.10863494873047\n",
      "Full Seq Acc: 0.2067573219537735\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04415156692266464\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.45464003706986\n",
      "Time: 0.04341483116149902\n",
      "\n",
      "Epoch: 77\n",
      "GPT loss: 91.664794921875\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.664794921875\n",
      "Full Seq Acc: 0.21164080500602722\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04304962232708931\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.43674158591791\n",
      "Time: 0.04336714744567871\n",
      "\n",
      "Epoch: 78\n",
      "GPT loss: 91.7637939453125\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.7637939453125\n",
      "Full Seq Acc: 0.21351012587547302\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04129793494939804\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.42001210951186\n",
      "Time: 0.04342222213745117\n",
      "\n",
      "Epoch: 79\n",
      "GPT loss: 89.02204895019531\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.02204895019531\n",
      "Full Seq Acc: 0.2139681577682495\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03915761783719063\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.3760324779187\n",
      "Time: 0.043341875076293945\n",
      "\n",
      "Epoch: 80\n",
      "GPT loss: 90.04136657714844\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.04136657714844\n",
      "Full Seq Acc: 0.21656769514083862\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.036878496408462524\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.342685818911\n",
      "Time: 0.043387651443481445\n",
      "\n",
      "Epoch: 81\n",
      "GPT loss: 90.58645629882812\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.58645629882812\n",
      "Full Seq Acc: 0.22244174778461456\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0374172180891037\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.31512352371017\n",
      "Time: 0.043399810791015625\n",
      "\n",
      "Epoch: 82\n",
      "GPT loss: 89.35330200195312\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.35330200195312\n",
      "Full Seq Acc: 0.22354227304458618\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03554970398545265\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.2755053084926\n",
      "Time: 0.04330873489379883\n",
      "\n",
      "Epoch: 83\n",
      "GPT loss: 85.5090560913086\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 85.5090560913086\n",
      "Full Seq Acc: 0.22419913113117218\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04937044531106949\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.19784081632075\n",
      "Time: 0.04350090026855469\n",
      "\n",
      "Epoch: 84\n",
      "GPT loss: 88.1018295288086\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.1018295288086\n",
      "Full Seq Acc: 0.22205200791358948\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03920922800898552\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.14688070344563\n",
      "Time: 0.043949127197265625\n",
      "\n",
      "Epoch: 85\n",
      "GPT loss: 87.77264404296875\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.77264404296875\n",
      "Full Seq Acc: 0.2268621325492859\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04011838138103485\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.09313833684085\n",
      "Time: 0.0438685417175293\n",
      "\n",
      "Epoch: 86\n",
      "GPT loss: 87.87046813964844\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.87046813964844\n",
      "Full Seq Acc: 0.22922472655773163\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04472213238477707\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.04091163486893\n",
      "Time: 0.043753862380981445\n",
      "\n",
      "Epoch: 87\n",
      "GPT loss: 89.8189926147461\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.8189926147461\n",
      "Full Seq Acc: 0.23913447558879852\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0366215743124485\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 93.00869244466772\n",
      "Time: 0.04369068145751953\n",
      "\n",
      "Epoch: 88\n",
      "GPT loss: 89.97176361083984\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.97176361083984\n",
      "Full Seq Acc: 0.23257723450660706\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03436988592147827\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.97832315632942\n",
      "Time: 0.043560028076171875\n",
      "\n",
      "Epoch: 89\n",
      "GPT loss: 89.68062591552734\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.68062591552734\n",
      "Full Seq Acc: 0.23683622479438782\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.035573121160268784\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.9453461839214\n",
      "Time: 0.04347085952758789\n",
      "\n",
      "Epoch: 90\n",
      "GPT loss: 88.56722259521484\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.56722259521484\n",
      "Full Seq Acc: 0.2343442589044571\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03502974286675453\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.90156494803433\n",
      "Time: 0.043641090393066406\n",
      "\n",
      "Epoch: 91\n",
      "GPT loss: 85.49278259277344\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 85.49278259277344\n",
      "Full Seq Acc: 0.23898929357528687\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03615257143974304\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.82747712448172\n",
      "Time: 0.04393506050109863\n",
      "\n",
      "Epoch: 92\n",
      "GPT loss: 87.74555969238281\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.74555969238281\n",
      "Full Seq Acc: 0.24201418459415436\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03293807804584503\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.77665795016073\n",
      "Time: 0.04351997375488281\n",
      "\n",
      "Epoch: 93\n",
      "GPT loss: 87.47154998779297\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.47154998779297\n",
      "Full Seq Acc: 0.2429635226726532\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.035139571875333786\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.72360687053705\n",
      "Time: 0.043613433837890625\n",
      "\n",
      "Epoch: 94\n",
      "GPT loss: 89.77363586425781\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.77363586425781\n",
      "Full Seq Acc: 0.24284863471984863\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03325650095939636\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.69410716047426\n",
      "Time: 0.04348587989807129\n",
      "\n",
      "Epoch: 95\n",
      "GPT loss: 85.15803527832031\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 85.15803527832031\n",
      "Full Seq Acc: 0.24163365364074707\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03166227042675018\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.61874644165272\n",
      "Time: 0.04363441467285156\n",
      "\n",
      "Epoch: 96\n",
      "GPT loss: 90.57597351074219\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.57597351074219\n",
      "Full Seq Acc: 0.24776974320411682\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.027415143325924873\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.59831871234361\n",
      "Time: 0.04371905326843262\n",
      "\n",
      "Epoch: 97\n",
      "GPT loss: 88.35063934326172\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.35063934326172\n",
      "Full Seq Acc: 0.2515496611595154\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.025691699236631393\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.55584191865279\n",
      "Time: 0.0437922477722168\n",
      "\n",
      "Epoch: 98\n",
      "GPT loss: 90.00045013427734\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.00045013427734\n",
      "Full Seq Acc: 0.2539729177951813\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03759398311376572\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.53028800080904\n",
      "Time: 0.044058799743652344\n",
      "\n",
      "Epoch: 99\n",
      "GPT loss: 89.36211395263672\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.36211395263672\n",
      "Full Seq Acc: 0.2521364390850067\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.037958115339279175\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.49860626032732\n",
      "Time: 0.04401445388793945\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100\n",
      "GPT loss: 86.0082015991211\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.0082015991211\n",
      "Full Seq Acc: 0.25461894273757935\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0398288331925869\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.43370221371526\n",
      "Time: 0.044524192810058594\n",
      "\n",
      "Epoch: 101\n",
      "GPT loss: 86.84423828125\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.84423828125\n",
      "Full Seq Acc: 0.2552184760570526\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.030154049396514893\n",
      "Mean Steps: 0.0\n",
      "Std Steps: 0.0\n",
      "Smoothed Loss: 92.37780757439062\n",
      "Time: 0.04427194595336914\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m train_data = [x.to(device) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m train_data]\n\u001b[32m     15\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) == \u001b[32m1\u001b[39m:\n\u001b[32m     20\u001b[39m     data = result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(model, train_data, mean_len, optimizer, scheduler, scaler, args)\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [total_loss]\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     safe_params = [\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [model, optimizer, scheduler]]\n\u001b[32m     22\u001b[39m backward_pass(model, total_loss_for_backward, optimizer, scaler, scheduler, args.grad_clip_max_norm)\n\u001b[32m     24\u001b[39m data = [GPT_loss, \u001b[32m0\u001b[39m, total_loss, full_seq_acc, ans_region_acc, ans_char_acc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/copy.py:222\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    220\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/copy.py:222\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    220\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/copy.py:222\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    220\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/copy.py:144\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    142\u001b[39m copier = \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33m__deepcopy__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     reductor = dispatch_table.get(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/site-packages/torch/_tensor.py:150\u001b[39m, in \u001b[36mTensor.__deepcopy__\u001b[39m\u001b[34m(self, memo)\u001b[39m\n\u001b[32m    141\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    142\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe default implementation of __deepcopy__() for wrapper subclasses \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    143\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33monly works for subclass types that implement clone() and for which \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdifferent type.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    148\u001b[39m         )\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     new_storage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_typed_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_deepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_quantized:\n\u001b[32m    152\u001b[39m         \u001b[38;5;66;03m# quantizer_params can be different type based on torch attribute\u001b[39;00m\n\u001b[32m    153\u001b[39m         quantizer_params: Union[\n\u001b[32m    154\u001b[39m             Tuple[torch.qscheme, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[32m    155\u001b[39m             Tuple[torch.qscheme, Tensor, Tensor, \u001b[38;5;28mint\u001b[39m],\n\u001b[32m    156\u001b[39m         ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/site-packages/torch/storage.py:1136\u001b[39m, in \u001b[36mTypedStorage._deepcopy\u001b[39m\u001b[34m(self, memo)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_deepcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, memo):\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_wrapped_storage(\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_untyped_storage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/copy.py:144\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    142\u001b[39m copier = \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33m__deepcopy__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     reductor = dispatch_table.get(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/site-packages/torch/storage.py:244\u001b[39m, in \u001b[36m_StorageBase.__deepcopy__\u001b[39m\u001b[34m(self, memo)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cdata \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mself\u001b[39m._cdata]\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m new_storage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m memo[\u001b[38;5;28mself\u001b[39m._cdata] = new_storage\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/site-packages/torch/storage.py:258\u001b[39m, in \u001b[36m_StorageBase.clone\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclone\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    257\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a copy of this storage.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.copy_(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/neuralode/lib/python3.13/site-packages/torch/storage.py:58\u001b[39m, in \u001b[36m_StorageBase.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Used when stashing FakeTensor device onto storage in torch.save(metadata_only=True)\u001b[39;00m\n\u001b[32m     56\u001b[39m _fake_device: _Optional[torch.device] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _int:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "record = np.zeros((epochs, 9))\n",
    "num_NaNs = 0\n",
    "smoothed_loss = None\n",
    "\n",
    "safe_params = [copy.deepcopy(i.state_dict()) for i in [model, optimizer, scheduler]]\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "for train_data in dataloader:\n",
    "    if epoch >= epochs:\n",
    "        break\n",
    "\n",
    "    train_data = [x.to(device) for x in train_data]\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    result = train_step(model, train_data, mean_len, optimizer, scheduler, scaler, args)\n",
    "\n",
    "    if len(result) == 1:\n",
    "        data = result\n",
    "        total_loss = data[0]\n",
    "        num_NaNs += 1\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(\"Instability detected\")\n",
    "        print(f\"Total Loss: {total_loss.item()}\\n\")\n",
    "        model.load_state_dict(safe_params[0])\n",
    "        optimizer.load_state_dict(safe_params[1])\n",
    "        scheduler.load_state_dict(safe_params[2])\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        continue\n",
    "\n",
    "    data, safe_params = result\n",
    "    smoothed_loss = 0.99 * smoothed_loss + 0.01 * data[2].item() if smoothed_loss is not None else data[2].item()\n",
    "    epoch = epoch + 1\n",
    "\n",
    "    record[epoch - 1, :-1] = data\n",
    "    record[epoch - 1, -1] = num_NaNs\n",
    "        \n",
    "    names = [\"GPT loss\", \"Energy Reg\", \"Total_loss\", \"Full Seq Acc\",\n",
    "             \"Ans Region Acc\", \"Ans Char Acc\", \"Mean Steps\", \"Std Steps\", \"Num NaNs\"]\n",
    "\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for name, value in zip(names, data):\n",
    "        print(f\"{name}: {value}\")\n",
    "    print(f\"Smoothed Loss: {smoothed_loss}\")\n",
    "    print(f\"Time: {time.time() - t0}\\n\")\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        #save_record(args.save_path, model, record, args, time_stamp)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_record(args.final_save_path, model, record, args, time_stamp, smoothed_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
