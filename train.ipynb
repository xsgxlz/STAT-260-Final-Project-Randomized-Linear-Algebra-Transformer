{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time stamp: 2025-05-09 17-28-43\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"/accounts/grad/zhangyunzhe2023/stat 260/STAT-260-Final-Project-Randomized-Linear-Algebra-Transformer\")\n",
    "from RLALLaMA3.LLaMA3 import ModelArgs, Transformer\n",
    "from RLALLaMA3.utils import (\n",
    "    linear_warmup_cosine_decay_multiplicative,\n",
    "    name_args,\n",
    "    Args,\n",
    ")\n",
    "from RLALLaMA3.tasks import single_answer_seq_loss, get_dataset\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "time_stamp = time.strftime(\"%Y-%m-%d %H-%M-%S\", time.localtime())\n",
    "print(f\"Time stamp: {time_stamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args Configuration:\n",
      "\n",
      "Training Parameters:\n",
      "  model_type:         node\n",
      "  standard_lr:        1.0e-03\n",
      "  standard_epoch:     160000\n",
      "  standard_warmup_steps: 4000\n",
      "  batch_size:         2048\n",
      "  min_lr:             1.0e-04\n",
      "  grad_clip_max_norm: 1.0\n",
      "  use_amp:            True\n",
      "  use_compile:        False\n",
      "\n",
      "Data Parameters:\n",
      "  task:               number_add\n",
      "  max_level:          20\n",
      "  random_seq_len:     True\n",
      "  number_range:       (0, 99)\n",
      "\n",
      "Model Architecture Parameters:\n",
      "  dim:                256\n",
      "  n_layers:           4\n",
      "  n_heads:            4\n",
      "  hidden_dim:         896\n",
      "\n",
      "RLA Parameters:\n",
      "  deterministic:      False\n",
      "  sketch_mode:        rademacher\n",
      "  \n",
      "    # For Attention Linear Layers (Wq, Wk, Wv, Wo):\n",
      "    attn_qkv_sample_exact_dim: 1\n",
      "    attn_qkv_projection_dim: 48\n",
      "    attn_out_sample_exact_dim: 1\n",
      "    attn_out_projection_dim: 48\n",
      "  \n",
      "    # For Feed-Forward Network (FFN) Linear Layers:\n",
      "    ffn_in_sample_exact_dim:   1\n",
      "    ffn_in_projection_dim:   48\n",
      "    ffn_out_sample_exact_dim:  1\n",
      "    ffn_out_projection_dim:  48\n",
      "  \n",
      "    # For Scaled Dot-Product Attention (SDPA) internal matmuls:\n",
      "    sdpa_qk_sample_exact_dim: 1\n",
      "    sdpa_qk_projection_dim: 24\n",
      "    sdpa_sv_sample_exact_dim: 1\n",
      "    sdpa_sv_projection_dim: 24\n",
      "\n",
      "Save Path Parameters:\n",
      "  save_path:          ckpt\n",
      "  final_save_path:    ckpt_final\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the arguments\n",
    "##args = parse_args()\n",
    "args = Args(\n",
    "    # Training parameters\n",
    "    standard_lr=1e-3,\n",
    "    standard_epoch=100000,\n",
    "    standard_warmup_steps=4000,\n",
    "    batch_size=2048,\n",
    "    min_lr=1e-4,\n",
    "    grad_clip_max_norm=1.0,\n",
    "    use_amp=True,\n",
    "    use_compile=False,\n",
    "    model_type=\"transformer\",\n",
    "\n",
    "    # Data parameters\n",
    "    task=\"number_add\",\n",
    "    max_level=20,\n",
    "    random_seq_len=True,\n",
    "    number_range=(0, 99),\n",
    "\n",
    "    # Model architecture parameters\n",
    "    dim=256,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    hidden_dim=896,\n",
    "\n",
    "    # --- Randomized Linear Algebra (RLA) Parameters ---\n",
    "    deterministic=False,\n",
    "    sketch_mode='rademacher',\n",
    "\n",
    "    # RLA for Attention Linear Layers (Wq, Wk, Wv, Wo)\n",
    "    # Old: attention_qkv_sketch_size=48\n",
    "    rla_attn_qkv_sample_exact_dim=1,\n",
    "    rla_attn_qkv_projection_dim=48,\n",
    "    # Old: attention_out_sketch_size=48\n",
    "    rla_attn_out_sample_exact_dim=1,\n",
    "    rla_attn_out_projection_dim=48,\n",
    "\n",
    "    # RLA for Feed-Forward Network (FFN) Linear Layers\n",
    "    # Old: feedforward_sketch_size_in=48\n",
    "    rla_ffn_in_sample_exact_dim=1,\n",
    "    rla_ffn_in_projection_dim=48,\n",
    "    # Old: feedforward_sketch_size_out=48\n",
    "    rla_ffn_out_sample_exact_dim=1,\n",
    "    rla_ffn_out_projection_dim=48,\n",
    "\n",
    "    # RLA for Scaled Dot-Product Attention (SDPA) internal matmuls\n",
    "    # Old: attention_score_sketch_size=24 (for QK^T)\n",
    "    rla_sdpa_qk_sample_exact_dim=1,\n",
    "    rla_sdpa_qk_projection_dim=24,\n",
    "    # Old: attention_weighed_sum_sketch_size=24 (for Scores@V)\n",
    "    rla_sdpa_sv_sample_exact_dim=1,\n",
    "    rla_sdpa_sv_projection_dim=24,\n",
    "\n",
    "    # Save path\n",
    "    save_path=\"ckpt\",\n",
    "    final_save_path=\"ckpt_final\",\n",
    ")\n",
    "\n",
    "\n",
    "print(args, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "dataset, collate_fn, vocab_size, max_seq_len = get_dataset(args.task,\n",
    "                                                           args.max_level,\n",
    "                                                           args.random_seq_len,\n",
    "                                                           args.number_range,\n",
    "                                                           nested_tensor=False,\n",
    "                                                           pad_to_longest=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, collate_fn=collate_fn,\n",
    "                                         num_workers=torch.get_num_threads(), pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sequence length: 34.377392578125\n",
      "Max sequence length: 66\n"
     ]
    }
   ],
   "source": [
    "def mean_seq_len(dataloader, num_samples=100):\n",
    "    \"\"\"\n",
    "    Calculate the mean sequence length of the dataset.\n",
    "    \"\"\"\n",
    "    total_len = 0\n",
    "    num_samples = min(num_samples, len(dataloader.dataset))\n",
    "    for i, x in enumerate(dataloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        total_len += x[1].float().mean().item()\n",
    "    return total_len / num_samples\n",
    "\n",
    "mean_len = mean_seq_len(dataloader)\n",
    "print(f\"Mean sequence length: {mean_len}\")\n",
    "print(f\"Max sequence length: {max_seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model\n",
    "\n",
    "transformer_args = ModelArgs(\n",
    "    # Standard model parameters from Args\n",
    "    dim=args.dim,\n",
    "    n_layers=args.n_layers,\n",
    "    n_heads=args.n_heads,\n",
    "    hidden_dim=args.hidden_dim, # ModelArgs.__post_init__ might adjust this if None\n",
    "    vocab_size=vocab_size,      # Assumed to be defined in the scope\n",
    "    max_seq_len=max_seq_len,    # Assumed to be defined in the scope\n",
    "\n",
    "    # Parameters that might be hardcoded or could be added to Args if needed\n",
    "    norm_eps=1e-5,              # Default in ModelArgs, can be overridden\n",
    "    rope_theta=500000.0,        # Default in ModelArgs, can be overridden\n",
    "    # n_kv_heads=args.n_kv_heads, # If you add n_kv_heads to Args and ModelArgs\n",
    "\n",
    "    # RLA parameters from Args\n",
    "    deterministic=args.deterministic,\n",
    "    sketch_mode=args.sketch_mode,\n",
    "\n",
    "    # RLA for Attention Linear Layers\n",
    "    rla_attn_qkv_sample_exact_dim=args.rla_attn_qkv_sample_exact_dim,\n",
    "    rla_attn_qkv_projection_dim=args.rla_attn_qkv_projection_dim,\n",
    "    rla_attn_out_sample_exact_dim=args.rla_attn_out_sample_exact_dim,\n",
    "    rla_attn_out_projection_dim=args.rla_attn_out_projection_dim,\n",
    "\n",
    "    # RLA for Feed-Forward Network (FFN) Linear Layers\n",
    "    rla_ffn_in_sample_exact_dim=args.rla_ffn_in_sample_exact_dim,\n",
    "    rla_ffn_in_projection_dim=args.rla_ffn_in_projection_dim,\n",
    "    rla_ffn_out_sample_exact_dim=args.rla_ffn_out_sample_exact_dim,\n",
    "    rla_ffn_out_projection_dim=args.rla_ffn_out_projection_dim,\n",
    "\n",
    "    # RLA for Scaled Dot-Product Attention (SDPA) internal matmuls\n",
    "    rla_sdpa_qk_sample_exact_dim=args.rla_sdpa_qk_sample_exact_dim,\n",
    "    rla_sdpa_qk_projection_dim=args.rla_sdpa_qk_projection_dim,\n",
    "    rla_sdpa_sv_sample_exact_dim=args.rla_sdpa_sv_sample_exact_dim,\n",
    "    rla_sdpa_sv_projection_dim=args.rla_sdpa_sv_projection_dim,\n",
    ")\n",
    "\n",
    "model = Transformer(params=transformer_args)\n",
    "\n",
    "model = model.to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(15, 256, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (attention): RLAAttention(\n",
       "        (wqkv): RLALinear(in_features=256, out_features=768, bias=False, sample_exact_dim=1, projection_dim=48, projection_mode='rademacher', deterministic=False)\n",
       "        (wo): RLALinear(in_features=256, out_features=256, bias=False, sample_exact_dim=1, projection_dim=48, projection_mode='rademacher', deterministic=False)\n",
       "      )\n",
       "      (feed_forward): RLAFeedForward(\n",
       "        (w13): RLALinear(in_features=256, out_features=1792, bias=False, sample_exact_dim=1, projection_dim=48, projection_mode='rademacher', deterministic=False)\n",
       "        (w2): RLALinear(in_features=896, out_features=256, bias=False, sample_exact_dim=1, projection_dim=48, projection_mode='rademacher', deterministic=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn_norm): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (output): Linear(in_features=256, out_features=15, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived Parameters:\n",
      "lr: 0.004\n",
      "warmup_steps: 1000\n",
      "epochs: 40000\n",
      "grad_clip_max_norm: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "standard_lr = args.standard_lr / 512\n",
    "standard_epoch = args.standard_epoch * 512\n",
    "standard_warmup_steps = args.standard_warmup_steps * 512\n",
    "batch_size = args.batch_size\n",
    "\n",
    "lr = standard_lr * batch_size\n",
    "warmup_steps = standard_warmup_steps // batch_size\n",
    "epochs = standard_epoch // batch_size\n",
    "\n",
    "print(\"Derived Parameters:\")\n",
    "print(f\"lr: {lr}\")\n",
    "print(f\"warmup_steps: {warmup_steps}\")\n",
    "print(f\"epochs: {epochs}\")\n",
    "print(f\"grad_clip_max_norm: {args.grad_clip_max_norm}\", end=\"\\n\\n\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, fused=True)\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer,\n",
    "            lr_lambda=lambda step: linear_warmup_cosine_decay_multiplicative(step, warmup_steps, epochs, args.min_lr))\n",
    "\n",
    "scaler = torch.amp.GradScaler(device, enabled=args.use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and arguments\n",
    "\n",
    "def save_record(path, model, record, args, time_stamp, extra_info=None):\n",
    "    dict_name, file_name = name_args(args, \"_\")\n",
    "\n",
    "    os.makedirs(f\"{path}/{dict_name}\", exist_ok=True)\n",
    "    file_name = file_name + f\"_{time_stamp}\"\n",
    "    if extra_info is not None:\n",
    "        file_name += f\"_{extra_info}\"\n",
    "    \n",
    "    record_dict = {\n",
    "        \"model\": model,\n",
    "        \"record\": record,\n",
    "        \"args\": args,\n",
    "        \"time_stamp\": time_stamp,\n",
    "    }\n",
    "        \n",
    "    torch.save(record_dict, f\"{path}/{dict_name}/{file_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backwards pass\n",
    "def backward_pass(model, loss, optimizer, scaler, scheduler, grad_clip_max_norm):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_output_error(transformer_output, gt_transformer_output, pad_mask):\n",
    "    \"\"\"\n",
    "    Calculate the error between the transformer output and the ground truth output.\n",
    "    \"\"\"\n",
    "    # Apply the padding mask to both outputs\n",
    "    pad_mask = pad_mask.unsqueeze(-1)\n",
    "    transformer_output = transformer_output * pad_mask\n",
    "    gt_transformer_output = gt_transformer_output * pad_mask\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    abs_error = (transformer_output - gt_transformer_output).square().sum()\n",
    "    relative_error = abs_error / gt_transformer_output.square().sum()\n",
    "    num_tokens = pad_mask.sum()\n",
    "    abs_error = abs_error / num_tokens / transformer_output.size(-1)\n",
    "    return abs_error, relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(disable=not args.use_compile)\n",
    "def train_step(model, train_data, mean_len, optimizer, scheduler, scaler, args, calculate_errors=False):\n",
    "    device = train_data[0].device\n",
    "    \n",
    "    with torch.autocast(device_type=device.type, dtype=torch.bfloat16, enabled=args.use_amp):\n",
    "        tokens, lengths, ans_starts, ans_lengths = train_data\n",
    "\n",
    "        if calculate_errors:\n",
    "            with torch.no_grad():\n",
    "                pad_mask = tokens[:, 1:] == 0\n",
    "                model.deterministic_mode(True)\n",
    "                _, gt_transformer_output = model(tokens[:, :-1], return_transformer_output=True)\n",
    "                model.deterministic_mode(args.deterministic)\n",
    "\n",
    "        pred, transformer_output = model(tokens[:, :-1], return_transformer_output=True)\n",
    "        transformer_output = transformer_output.detach()\n",
    "\n",
    "        if calculate_errors:\n",
    "            with torch.no_grad():\n",
    "                abs_error, relative_error = transformer_output_error(transformer_output, gt_transformer_output, pad_mask)\n",
    "                abs_error, relative_error = abs_error.detach(), relative_error.detach()\n",
    "        \n",
    "        result = single_answer_seq_loss(pred, tokens, lengths, ans_starts, ans_lengths)\n",
    "        GPT_loss, full_seq_acc, ans_region_acc, ans_char_acc = result\n",
    "        # Normalize the GPT loss by the batch size but not the sequence length\n",
    "        GPT_loss = GPT_loss / args.batch_size\n",
    "        total_loss = GPT_loss\n",
    "        total_loss_for_backward = total_loss / mean_len\n",
    "    \n",
    "    if torch.isnan(total_loss) or torch.isinf(total_loss):# or (total_loss > smoothed_loss * 1.1):\n",
    "        return [total_loss]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        safe_params = [copy.deepcopy(i.state_dict()) for i in [model, optimizer, scheduler]]\n",
    "\n",
    "    backward_pass(model, total_loss_for_backward, optimizer, scaler, scheduler, args.grad_clip_max_norm)\n",
    "    \n",
    "    data = [GPT_loss, 0, total_loss, full_seq_acc, ans_region_acc, ans_char_acc]\n",
    "\n",
    "    if calculate_errors:\n",
    "        data = data + [abs_error, relative_error]\n",
    "    else:\n",
    "        data = data + [0, 0]\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        data = torch.tensor(data).cpu().numpy()\n",
    "\n",
    "    return data, safe_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "GPT loss: 96.86146545410156\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 96.86146545410156\n",
      "Full Seq Acc: 0.06769181787967682\n",
      "Ans Region Acc: 0.00048828125\n",
      "Ans Char Acc: 0.0640089139342308\n",
      "Abs Error: 1.9858959913253784\n",
      "Rel Error: 1.9860275983810425\n",
      "Smoothed Loss: 96.86146545410156\n",
      "Time: 7.67026948928833\n",
      "\n",
      "Epoch: 2\n",
      "GPT loss: 94.7293701171875\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.7293701171875\n",
      "Full Seq Acc: 0.11876743286848068\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.053491827100515366\n",
      "Abs Error: 1.925817847251892\n",
      "Rel Error: 1.9414554834365845\n",
      "Smoothed Loss: 96.84014450073242\n",
      "Time: 0.6268966197967529\n",
      "\n",
      "Epoch: 3\n",
      "GPT loss: 95.77971649169922\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 95.77971649169922\n",
      "Full Seq Acc: 0.11985523253679276\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05146155506372452\n",
      "Abs Error: 1.9227657318115234\n",
      "Rel Error: 1.9384046792984009\n",
      "Smoothed Loss: 96.82954022064209\n",
      "Time: 0.628669261932373\n",
      "\n",
      "Epoch: 4\n",
      "GPT loss: 94.82600402832031\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.82600402832031\n",
      "Full Seq Acc: 0.12356041371822357\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05309297889471054\n",
      "Abs Error: 1.925888180732727\n",
      "Rel Error: 1.9415966272354126\n",
      "Smoothed Loss: 96.80950485871887\n",
      "Time: 0.6252322196960449\n",
      "\n",
      "Epoch: 5\n",
      "GPT loss: 96.54609680175781\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 96.54609680175781\n",
      "Full Seq Acc: 0.11913266777992249\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04960610717535019\n",
      "Abs Error: 1.9273570775985718\n",
      "Rel Error: 1.9431294202804565\n",
      "Smoothed Loss: 96.80687077814926\n",
      "Time: 0.6747872829437256\n",
      "\n",
      "Epoch: 6\n",
      "GPT loss: 94.93999481201172\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.93999481201172\n",
      "Full Seq Acc: 0.12160102277994156\n",
      "Ans Region Acc: 0.00048828125\n",
      "Ans Char Acc: 0.04945732653141022\n",
      "Abs Error: 1.9284192323684692\n",
      "Rel Error: 1.9442644119262695\n",
      "Smoothed Loss: 96.78820201848788\n",
      "Time: 0.6261281967163086\n",
      "\n",
      "Epoch: 7\n",
      "GPT loss: 95.5715103149414\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 95.5715103149414\n",
      "Full Seq Acc: 0.1232980340719223\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.052165355533361435\n",
      "Abs Error: 1.921870470046997\n",
      "Rel Error: 1.937743902206421\n",
      "Smoothed Loss: 96.77603510145242\n",
      "Time: 0.6272976398468018\n",
      "\n",
      "Epoch: 8\n",
      "GPT loss: 95.0777587890625\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 95.0777587890625\n",
      "Full Seq Acc: 0.12342462688684464\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04851619526743889\n",
      "Abs Error: 1.9237722158432007\n",
      "Rel Error: 1.9397417306900024\n",
      "Smoothed Loss: 96.75905233832853\n",
      "Time: 0.6249902248382568\n",
      "\n",
      "Epoch: 9\n",
      "GPT loss: 94.33184814453125\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.33184814453125\n",
      "Full Seq Acc: 0.12387333065271378\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.048524510115385056\n",
      "Abs Error: 1.9168109893798828\n",
      "Rel Error: 1.9328242540359497\n",
      "Smoothed Loss: 96.73478029639057\n",
      "Time: 0.6248583793640137\n",
      "\n",
      "Epoch: 10\n",
      "GPT loss: 94.89378356933594\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.89378356933594\n",
      "Full Seq Acc: 0.12527476251125336\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04978488013148308\n",
      "Abs Error: 1.9182240962982178\n",
      "Rel Error: 1.9343661069869995\n",
      "Smoothed Loss: 96.71637032912002\n",
      "Time: 0.6251769065856934\n",
      "\n",
      "Epoch: 11\n",
      "GPT loss: 93.716064453125\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.716064453125\n",
      "Full Seq Acc: 0.12346314638853073\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04960610717535019\n",
      "Abs Error: 1.9169143438339233\n",
      "Rel Error: 1.9331707954406738\n",
      "Smoothed Loss: 96.68636727036007\n",
      "Time: 0.6241686344146729\n",
      "\n",
      "Epoch: 12\n",
      "GPT loss: 93.4014663696289\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.4014663696289\n",
      "Full Seq Acc: 0.12392367422580719\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.05089561268687248\n",
      "Abs Error: 1.916161298751831\n",
      "Rel Error: 1.9325438737869263\n",
      "Smoothed Loss: 96.65351826135276\n",
      "Time: 0.6261191368103027\n",
      "\n",
      "Epoch: 13\n",
      "GPT loss: 94.09807586669922\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.09807586669922\n",
      "Full Seq Acc: 0.12585313618183136\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04916820675134659\n",
      "Abs Error: 1.9155082702636719\n",
      "Rel Error: 1.9320284128189087\n",
      "Smoothed Loss: 96.62796383740623\n",
      "Time: 0.6256628036499023\n",
      "\n",
      "Epoch: 14\n",
      "GPT loss: 95.45439147949219\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 95.45439147949219\n",
      "Full Seq Acc: 0.12967538833618164\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04491609334945679\n",
      "Abs Error: 1.9146796464920044\n",
      "Rel Error: 1.9313536882400513\n",
      "Smoothed Loss: 96.61622811382709\n",
      "Time: 0.6269364356994629\n",
      "\n",
      "Epoch: 15\n",
      "GPT loss: 94.62925720214844\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.62925720214844\n",
      "Full Seq Acc: 0.1304880976676941\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04929577559232712\n",
      "Abs Error: 1.9136024713516235\n",
      "Rel Error: 1.9304391145706177\n",
      "Smoothed Loss: 96.59635840471032\n",
      "Time: 0.6266074180603027\n",
      "\n",
      "Epoch: 16\n",
      "GPT loss: 92.00955963134766\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.00955963134766\n",
      "Full Seq Acc: 0.12931634485721588\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04374690726399422\n",
      "Abs Error: 1.907975673675537\n",
      "Rel Error: 1.924942135810852\n",
      "Smoothed Loss: 96.55049041697669\n",
      "Time: 0.6269147396087646\n",
      "\n",
      "Epoch: 17\n",
      "GPT loss: 94.12028503417969\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.12028503417969\n",
      "Full Seq Acc: 0.13253828883171082\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04973466694355011\n",
      "Abs Error: 1.9117534160614014\n",
      "Rel Error: 1.92894446849823\n",
      "Smoothed Loss: 96.52618836314872\n",
      "Time: 0.6274223327636719\n",
      "\n",
      "Epoch: 18\n",
      "GPT loss: 92.5736083984375\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.5736083984375\n",
      "Full Seq Acc: 0.13464108109474182\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04846467077732086\n",
      "Abs Error: 1.9062492847442627\n",
      "Rel Error: 1.923580288887024\n",
      "Smoothed Loss: 96.48666256350161\n",
      "Time: 0.6276853084564209\n",
      "\n",
      "Epoch: 19\n",
      "GPT loss: 94.46317291259766\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.46317291259766\n",
      "Full Seq Acc: 0.1350499540567398\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.043236564844846725\n",
      "Abs Error: 1.9038671255111694\n",
      "Rel Error: 1.921388030052185\n",
      "Smoothed Loss: 96.46642766699257\n",
      "Time: 0.627887487411499\n",
      "\n",
      "Epoch: 20\n",
      "GPT loss: 93.56399536132812\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.56399536132812\n",
      "Full Seq Acc: 0.13923726975917816\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.046212777495384216\n",
      "Abs Error: 1.8962937593460083\n",
      "Rel Error: 1.913964033126831\n",
      "Smoothed Loss: 96.43740334393593\n",
      "Time: 0.6281569004058838\n",
      "\n",
      "Epoch: 21\n",
      "GPT loss: 94.08302307128906\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.08302307128906\n",
      "Full Seq Acc: 0.14083971083164215\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0500987209379673\n",
      "Abs Error: 1.9003715515136719\n",
      "Rel Error: 1.9183051586151123\n",
      "Smoothed Loss: 96.41385954120946\n",
      "Time: 0.6284730434417725\n",
      "\n",
      "Epoch: 22\n",
      "GPT loss: 95.23213195800781\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 95.23213195800781\n",
      "Full Seq Acc: 0.14511804282665253\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.050368551164865494\n",
      "Abs Error: 1.8934603929519653\n",
      "Rel Error: 1.9115535020828247\n",
      "Smoothed Loss: 96.40204226537745\n",
      "Time: 0.6282587051391602\n",
      "\n",
      "Epoch: 23\n",
      "GPT loss: 92.15621185302734\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.15621185302734\n",
      "Full Seq Acc: 0.14212921261787415\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04358024522662163\n",
      "Abs Error: 1.8898435831069946\n",
      "Rel Error: 1.9081374406814575\n",
      "Smoothed Loss: 96.35958396125395\n",
      "Time: 0.6284301280975342\n",
      "\n",
      "Epoch: 24\n",
      "GPT loss: 93.01554107666016\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.01554107666016\n",
      "Full Seq Acc: 0.14752492308616638\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04191838204860687\n",
      "Abs Error: 1.8829278945922852\n",
      "Rel Error: 1.9013934135437012\n",
      "Smoothed Loss: 96.32614353240801\n",
      "Time: 0.6287007331848145\n",
      "\n",
      "Epoch: 25\n",
      "GPT loss: 94.16119384765625\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 94.16119384765625\n",
      "Full Seq Acc: 0.14905744791030884\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04208194836974144\n",
      "Abs Error: 1.8790807723999023\n",
      "Rel Error: 1.8977537155151367\n",
      "Smoothed Loss: 96.3044940355605\n",
      "Time: 0.628831148147583\n",
      "\n",
      "Epoch: 26\n",
      "GPT loss: 92.45110321044922\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.45110321044922\n",
      "Full Seq Acc: 0.14878413081169128\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04287126287817955\n",
      "Abs Error: 1.8789727687835693\n",
      "Rel Error: 1.8979078531265259\n",
      "Smoothed Loss: 96.26596012730938\n",
      "Time: 0.6288392543792725\n",
      "\n",
      "Epoch: 27\n",
      "GPT loss: 95.04592895507812\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 95.04592895507812\n",
      "Full Seq Acc: 0.15301121771335602\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.043923500925302505\n",
      "Abs Error: 1.8732540607452393\n",
      "Rel Error: 1.8923883438110352\n",
      "Smoothed Loss: 96.25375981558707\n",
      "Time: 0.629692792892456\n",
      "\n",
      "Epoch: 28\n",
      "GPT loss: 91.970703125\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.970703125\n",
      "Full Seq Acc: 0.15512089431285858\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.039377856999635696\n",
      "Abs Error: 1.8680496215820312\n",
      "Rel Error: 1.8873993158340454\n",
      "Smoothed Loss: 96.2109292486812\n",
      "Time: 0.6289956569671631\n",
      "\n",
      "Epoch: 29\n",
      "GPT loss: 91.49188995361328\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.49188995361328\n",
      "Full Seq Acc: 0.15765441954135895\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04170792177319527\n",
      "Abs Error: 1.864504098892212\n",
      "Rel Error: 1.8840739727020264\n",
      "Smoothed Loss: 96.16373885573051\n",
      "Time: 0.6295638084411621\n",
      "\n",
      "Epoch: 30\n",
      "GPT loss: 93.03870391845703\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 93.03870391845703\n",
      "Full Seq Acc: 0.15983335673809052\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.04204685613512993\n",
      "Abs Error: 1.8559017181396484\n",
      "Rel Error: 1.8756322860717773\n",
      "Smoothed Loss: 96.13248850635777\n",
      "Time: 0.6303138732910156\n",
      "\n",
      "Epoch: 31\n",
      "GPT loss: 92.42936706542969\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.42936706542969\n",
      "Full Seq Acc: 0.16267766058444977\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03761253133416176\n",
      "Abs Error: 1.8509981632232666\n",
      "Rel Error: 1.8709524869918823\n",
      "Smoothed Loss: 96.09545729194849\n",
      "Time: 0.629791259765625\n",
      "\n",
      "Epoch: 32\n",
      "GPT loss: 92.7419662475586\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.7419662475586\n",
      "Full Seq Acc: 0.1658630222082138\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03504442423582077\n",
      "Abs Error: 1.8442845344543457\n",
      "Rel Error: 1.8644343614578247\n",
      "Smoothed Loss: 96.06192238150459\n",
      "Time: 0.6307158470153809\n",
      "\n",
      "Epoch: 33\n",
      "GPT loss: 92.21379852294922\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.21379852294922\n",
      "Full Seq Acc: 0.1697283387184143\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.038191452622413635\n",
      "Abs Error: 1.8397154808044434\n",
      "Rel Error: 1.8600865602493286\n",
      "Smoothed Loss: 96.02344114291904\n",
      "Time: 0.6302757263183594\n",
      "\n",
      "Epoch: 34\n",
      "GPT loss: 91.51350402832031\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.51350402832031\n",
      "Full Seq Acc: 0.17157909274101257\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0391240157186985\n",
      "Abs Error: 1.8294488191604614\n",
      "Rel Error: 1.8499752283096313\n",
      "Smoothed Loss: 95.97834177177305\n",
      "Time: 0.6290743350982666\n",
      "\n",
      "Epoch: 35\n",
      "GPT loss: 91.33888244628906\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.33888244628906\n",
      "Full Seq Acc: 0.17414721846580505\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0377451591193676\n",
      "Abs Error: 1.8258507251739502\n",
      "Rel Error: 1.8466252088546753\n",
      "Smoothed Loss: 95.9319471785182\n",
      "Time: 0.6302323341369629\n",
      "\n",
      "Epoch: 36\n",
      "GPT loss: 92.53715515136719\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.53715515136719\n",
      "Full Seq Acc: 0.1787562221288681\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.03634795546531677\n",
      "Abs Error: 1.819491982460022\n",
      "Rel Error: 1.8404736518859863\n",
      "Smoothed Loss: 95.8979992582467\n",
      "Time: 0.6304850578308105\n",
      "\n",
      "Epoch: 37\n",
      "GPT loss: 92.0933609008789\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.0933609008789\n",
      "Full Seq Acc: 0.180861696600914\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.032333701848983765\n",
      "Abs Error: 1.8077620267868042\n",
      "Rel Error: 1.8288915157318115\n",
      "Smoothed Loss: 95.85995287467301\n",
      "Time: 0.6311540603637695\n",
      "\n",
      "Epoch: 38\n",
      "GPT loss: 91.76952362060547\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.76952362060547\n",
      "Full Seq Acc: 0.18519562482833862\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.031072748824954033\n",
      "Abs Error: 1.8078416585922241\n",
      "Rel Error: 1.829256296157837\n",
      "Smoothed Loss: 95.81904858213234\n",
      "Time: 0.6299772262573242\n",
      "\n",
      "Epoch: 39\n",
      "GPT loss: 91.25567626953125\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.25567626953125\n",
      "Full Seq Acc: 0.1874920278787613\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.028514999896287918\n",
      "Abs Error: 1.7977443933486938\n",
      "Rel Error: 1.819344401359558\n",
      "Smoothed Loss: 95.77341485900634\n",
      "Time: 0.6298015117645264\n",
      "\n",
      "Epoch: 40\n",
      "GPT loss: 92.29466247558594\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 92.29466247558594\n",
      "Full Seq Acc: 0.19016201794147491\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.029716400429606438\n",
      "Abs Error: 1.7883473634719849\n",
      "Rel Error: 1.810097098350525\n",
      "Smoothed Loss: 95.73862733517214\n",
      "Time: 0.6308639049530029\n",
      "\n",
      "Epoch: 41\n",
      "GPT loss: 90.79230499267578\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.79230499267578\n",
      "Full Seq Acc: 0.19152715802192688\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.027705948799848557\n",
      "Abs Error: 1.7794827222824097\n",
      "Rel Error: 1.801415205001831\n",
      "Smoothed Loss: 95.68916411174717\n",
      "Time: 0.6305947303771973\n",
      "\n",
      "Epoch: 42\n",
      "GPT loss: 91.85357666015625\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.85357666015625\n",
      "Full Seq Acc: 0.19395391643047333\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.02777436561882496\n",
      "Abs Error: 1.7710391283035278\n",
      "Rel Error: 1.7931482791900635\n",
      "Smoothed Loss: 95.65080823723127\n",
      "Time: 0.6303653717041016\n",
      "\n",
      "Epoch: 43\n",
      "GPT loss: 91.0100326538086\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.0100326538086\n",
      "Full Seq Acc: 0.20108488202095032\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.030003705993294716\n",
      "Abs Error: 1.7690783739089966\n",
      "Rel Error: 1.7915118932724\n",
      "Smoothed Loss: 95.60440048139705\n",
      "Time: 0.6312887668609619\n",
      "\n",
      "Epoch: 44\n",
      "GPT loss: 90.56250762939453\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.56250762939453\n",
      "Full Seq Acc: 0.2001161128282547\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.024775052443146706\n",
      "Abs Error: 1.7640398740768433\n",
      "Rel Error: 1.786704421043396\n",
      "Smoothed Loss: 95.55398155287702\n",
      "Time: 0.6318972110748291\n",
      "\n",
      "Epoch: 45\n",
      "GPT loss: 89.56648254394531\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.56648254394531\n",
      "Full Seq Acc: 0.2043076902627945\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.02683650702238083\n",
      "Abs Error: 1.7571306228637695\n",
      "Rel Error: 1.7800637483596802\n",
      "Smoothed Loss: 95.4941065627877\n",
      "Time: 0.6314163208007812\n",
      "\n",
      "Epoch: 46\n",
      "GPT loss: 89.82474517822266\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.82474517822266\n",
      "Full Seq Acc: 0.2082621306180954\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.025095809251070023\n",
      "Abs Error: 1.7420490980148315\n",
      "Rel Error: 1.7650866508483887\n",
      "Smoothed Loss: 95.43741294894205\n",
      "Time: 0.6307015419006348\n",
      "\n",
      "Epoch: 47\n",
      "GPT loss: 88.18458557128906\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.18458557128906\n",
      "Full Seq Acc: 0.20816479623317719\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.02432098798453808\n",
      "Abs Error: 1.7299302816390991\n",
      "Rel Error: 1.7531917095184326\n",
      "Smoothed Loss: 95.36488467516553\n",
      "Time: 0.631643533706665\n",
      "\n",
      "Epoch: 48\n",
      "GPT loss: 90.79993438720703\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.79993438720703\n",
      "Full Seq Acc: 0.21084116399288177\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.023747999221086502\n",
      "Abs Error: 1.7305793762207031\n",
      "Rel Error: 1.7541720867156982\n",
      "Smoothed Loss: 95.31923517228593\n",
      "Time: 0.6318304538726807\n",
      "\n",
      "Epoch: 49\n",
      "GPT loss: 89.45240020751953\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.45240020751953\n",
      "Full Seq Acc: 0.21181179583072662\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.021851852536201477\n",
      "Abs Error: 1.7239081859588623\n",
      "Rel Error: 1.747751235961914\n",
      "Smoothed Loss: 95.26056682263827\n",
      "Time: 0.6314504146575928\n",
      "\n",
      "Epoch: 50\n",
      "GPT loss: 91.08052062988281\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 91.08052062988281\n",
      "Full Seq Acc: 0.2159552425146103\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.020054133608937263\n",
      "Abs Error: 1.7081013917922974\n",
      "Rel Error: 1.7321041822433472\n",
      "Smoothed Loss: 95.21876636071072\n",
      "Time: 0.6331448554992676\n",
      "\n",
      "Epoch: 51\n",
      "GPT loss: 90.24089813232422\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.24089813232422\n",
      "Full Seq Acc: 0.2158379852771759\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.01946531981229782\n",
      "Abs Error: 1.7012048959732056\n",
      "Rel Error: 1.7255066633224487\n",
      "Smoothed Loss: 95.16898767842686\n",
      "Time: 0.6314525604248047\n",
      "\n",
      "Epoch: 52\n",
      "GPT loss: 90.09599304199219\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.09599304199219\n",
      "Full Seq Acc: 0.21948786079883575\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.022151120007038116\n",
      "Abs Error: 1.6932204961776733\n",
      "Rel Error: 1.7178319692611694\n",
      "Smoothed Loss: 95.1182577320625\n",
      "Time: 0.6335883140563965\n",
      "\n",
      "Epoch: 53\n",
      "GPT loss: 89.19065856933594\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.19065856933594\n",
      "Full Seq Acc: 0.22076687216758728\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.018842365592718124\n",
      "Abs Error: 1.679659128189087\n",
      "Rel Error: 1.7044459581375122\n",
      "Smoothed Loss: 95.05898174043524\n",
      "Time: 0.6322507858276367\n",
      "\n",
      "Epoch: 54\n",
      "GPT loss: 86.53633117675781\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.53633117675781\n",
      "Full Seq Acc: 0.22380660474300385\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.020071864128112793\n",
      "Abs Error: 1.678557276725769\n",
      "Rel Error: 1.7037911415100098\n",
      "Smoothed Loss: 94.97375523479847\n",
      "Time: 0.6316280364990234\n",
      "\n",
      "Epoch: 55\n",
      "GPT loss: 90.89724731445312\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 90.89724731445312\n",
      "Full Seq Acc: 0.2263098806142807\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.016461916267871857\n",
      "Abs Error: 1.672587513923645\n",
      "Rel Error: 1.698155164718628\n",
      "Smoothed Loss: 94.93299015559502\n",
      "Time: 0.6325643062591553\n",
      "\n",
      "Epoch: 56\n",
      "GPT loss: 88.90008544921875\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.90008544921875\n",
      "Full Seq Acc: 0.22598348557949066\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.017126664519309998\n",
      "Abs Error: 1.6584818363189697\n",
      "Rel Error: 1.684263825416565\n",
      "Smoothed Loss: 94.87266110853125\n",
      "Time: 0.632990837097168\n",
      "\n",
      "Epoch: 57\n",
      "GPT loss: 87.82463073730469\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.82463073730469\n",
      "Full Seq Acc: 0.2263237088918686\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.017775584012269974\n",
      "Abs Error: 1.6566115617752075\n",
      "Rel Error: 1.68282949924469\n",
      "Smoothed Loss: 94.80218080481897\n",
      "Time: 0.6320035457611084\n",
      "\n",
      "Epoch: 58\n",
      "GPT loss: 87.83082580566406\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.83082580566406\n",
      "Full Seq Acc: 0.2277505248785019\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.016765285283327103\n",
      "Abs Error: 1.6469751596450806\n",
      "Rel Error: 1.673479437828064\n",
      "Smoothed Loss: 94.73246725482741\n",
      "Time: 0.6328778266906738\n",
      "\n",
      "Epoch: 59\n",
      "GPT loss: 87.87338256835938\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.87338256835938\n",
      "Full Seq Acc: 0.2303004264831543\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.015941670164465904\n",
      "Abs Error: 1.629665493965149\n",
      "Rel Error: 1.6563279628753662\n",
      "Smoothed Loss: 94.66387640796273\n",
      "Time: 0.6333436965942383\n",
      "\n",
      "Epoch: 60\n",
      "GPT loss: 86.93429565429688\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.93429565429688\n",
      "Full Seq Acc: 0.23259399831295013\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.01669345796108246\n",
      "Abs Error: 1.6296277046203613\n",
      "Rel Error: 1.6567875146865845\n",
      "Smoothed Loss: 94.58658060042607\n",
      "Time: 0.6329350471496582\n",
      "\n",
      "Epoch: 61\n",
      "GPT loss: 88.52957916259766\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.52957916259766\n",
      "Full Seq Acc: 0.23250257968902588\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.015407371334731579\n",
      "Abs Error: 1.6214910745620728\n",
      "Rel Error: 1.6489795446395874\n",
      "Smoothed Loss: 94.52601058604779\n",
      "Time: 0.6325767040252686\n",
      "\n",
      "Epoch: 62\n",
      "GPT loss: 89.43892669677734\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.43892669677734\n",
      "Full Seq Acc: 0.23370400071144104\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.015239031985402107\n",
      "Abs Error: 1.609558343887329\n",
      "Rel Error: 1.63724684715271\n",
      "Smoothed Loss: 94.47513974715508\n",
      "Time: 0.6330556869506836\n",
      "\n",
      "Epoch: 63\n",
      "GPT loss: 88.2867660522461\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.2867660522461\n",
      "Full Seq Acc: 0.23492884635925293\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.014433751814067364\n",
      "Abs Error: 1.606244444847107\n",
      "Rel Error: 1.6343375444412231\n",
      "Smoothed Loss: 94.413256010206\n",
      "Time: 0.632728099822998\n",
      "\n",
      "Epoch: 64\n",
      "GPT loss: 88.43286895751953\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.43286895751953\n",
      "Full Seq Acc: 0.23480026423931122\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.01610523648560047\n",
      "Abs Error: 1.591678261756897\n",
      "Rel Error: 1.6199265718460083\n",
      "Smoothed Loss: 94.35345213967913\n",
      "Time: 0.6339888572692871\n",
      "\n",
      "Epoch: 65\n",
      "GPT loss: 88.07837677001953\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.07837677001953\n",
      "Full Seq Acc: 0.23665879666805267\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.012855377048254013\n",
      "Abs Error: 1.5870776176452637\n",
      "Rel Error: 1.6156944036483765\n",
      "Smoothed Loss: 94.29070138598253\n",
      "Time: 0.6341817378997803\n",
      "\n",
      "Epoch: 66\n",
      "GPT loss: 87.31859588623047\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.31859588623047\n",
      "Full Seq Acc: 0.23693017661571503\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.016148915514349937\n",
      "Abs Error: 1.577060341835022\n",
      "Rel Error: 1.605920433998108\n",
      "Smoothed Loss: 94.22098033098501\n",
      "Time: 0.633342981338501\n",
      "\n",
      "Epoch: 67\n",
      "GPT loss: 89.17108154296875\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 89.17108154296875\n",
      "Full Seq Acc: 0.2390439361333847\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.014233129099011421\n",
      "Abs Error: 1.5645290613174438\n",
      "Rel Error: 1.593550682067871\n",
      "Smoothed Loss: 94.17048134310485\n",
      "Time: 0.632521390914917\n",
      "\n",
      "Epoch: 68\n",
      "GPT loss: 87.72405242919922\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.72405242919922\n",
      "Full Seq Acc: 0.23976542055606842\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.015265295282006264\n",
      "Abs Error: 1.5637415647506714\n",
      "Rel Error: 1.5931555032730103\n",
      "Smoothed Loss: 94.1060170539658\n",
      "Time: 0.6345276832580566\n",
      "\n",
      "Epoch: 69\n",
      "GPT loss: 87.97806549072266\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.97806549072266\n",
      "Full Seq Acc: 0.23968513309955597\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.011064666323363781\n",
      "Abs Error: 1.5485349893569946\n",
      "Rel Error: 1.5780225992202759\n",
      "Smoothed Loss: 94.04473753833337\n",
      "Time: 0.6334712505340576\n",
      "\n",
      "Epoch: 70\n",
      "GPT loss: 88.75275421142578\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.75275421142578\n",
      "Full Seq Acc: 0.24028578400611877\n",
      "Ans Region Acc: 0.00048828125\n",
      "Ans Char Acc: 0.013162750750780106\n",
      "Abs Error: 1.540701150894165\n",
      "Rel Error: 1.5704231262207031\n",
      "Smoothed Loss: 93.99181770506429\n",
      "Time: 0.6334443092346191\n",
      "\n",
      "Epoch: 71\n",
      "GPT loss: 86.79003143310547\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.79003143310547\n",
      "Full Seq Acc: 0.2418709397315979\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.013366336934268475\n",
      "Abs Error: 1.5268795490264893\n",
      "Rel Error: 1.556718349456787\n",
      "Smoothed Loss: 93.9197998423447\n",
      "Time: 0.6336154937744141\n",
      "\n",
      "Epoch: 72\n",
      "GPT loss: 85.83193969726562\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 85.83193969726562\n",
      "Full Seq Acc: 0.24275867640972137\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.01423619594424963\n",
      "Abs Error: 1.5163847208023071\n",
      "Rel Error: 1.5463100671768188\n",
      "Smoothed Loss: 93.83892124089391\n",
      "Time: 0.6359643936157227\n",
      "\n",
      "Epoch: 73\n",
      "GPT loss: 85.10957336425781\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 85.10957336425781\n",
      "Full Seq Acc: 0.24322010576725006\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.013585277833044529\n",
      "Abs Error: 1.5111966133117676\n",
      "Rel Error: 1.5413860082626343\n",
      "Smoothed Loss: 93.75162776212755\n",
      "Time: 0.6344234943389893\n",
      "\n",
      "Epoch: 74\n",
      "GPT loss: 88.8257064819336\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 88.8257064819336\n",
      "Full Seq Acc: 0.2451445460319519\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.011922321282327175\n",
      "Abs Error: 1.4960927963256836\n",
      "Rel Error: 1.5262619256973267\n",
      "Smoothed Loss: 93.70236854932563\n",
      "Time: 0.6347904205322266\n",
      "\n",
      "Epoch: 75\n",
      "GPT loss: 86.96908569335938\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.96908569335938\n",
      "Full Seq Acc: 0.2441137731075287\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.01184600219130516\n",
      "Abs Error: 1.483465313911438\n",
      "Rel Error: 1.5136957168579102\n",
      "Smoothed Loss: 93.63503572076596\n",
      "Time: 0.6344704627990723\n",
      "\n",
      "Epoch: 76\n",
      "GPT loss: 87.74945068359375\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.74945068359375\n",
      "Full Seq Acc: 0.24841780960559845\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.010199066251516342\n",
      "Abs Error: 1.4754478931427002\n",
      "Rel Error: 1.505785346031189\n",
      "Smoothed Loss: 93.57617987039424\n",
      "Time: 0.6357254981994629\n",
      "\n",
      "Epoch: 77\n",
      "GPT loss: 85.93270111083984\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 85.93270111083984\n",
      "Full Seq Acc: 0.2457461953163147\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.010730142705142498\n",
      "Abs Error: 1.4619081020355225\n",
      "Rel Error: 1.492260456085205\n",
      "Smoothed Loss: 93.49974508279871\n",
      "Time: 0.634669303894043\n",
      "\n",
      "Epoch: 78\n",
      "GPT loss: 86.733154296875\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.733154296875\n",
      "Full Seq Acc: 0.2513035833835602\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.011825572699308395\n",
      "Abs Error: 1.4522289037704468\n",
      "Rel Error: 1.4826195240020752\n",
      "Smoothed Loss: 93.43207917493947\n",
      "Time: 0.6350343227386475\n",
      "\n",
      "Epoch: 79\n",
      "GPT loss: 87.03267669677734\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 87.03267669677734\n",
      "Full Seq Acc: 0.250471830368042\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.008118080906569958\n",
      "Abs Error: 1.4363280534744263\n",
      "Rel Error: 1.4667023420333862\n",
      "Smoothed Loss: 93.36808515015785\n",
      "Time: 0.6344504356384277\n",
      "\n",
      "Epoch: 80\n",
      "GPT loss: 86.64956665039062\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.64956665039062\n",
      "Full Seq Acc: 0.25245943665504456\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.010751359164714813\n",
      "Abs Error: 1.4246872663497925\n",
      "Rel Error: 1.4550886154174805\n",
      "Smoothed Loss: 93.30089996516018\n",
      "Time: 0.6342651844024658\n",
      "\n",
      "Epoch: 81\n",
      "GPT loss: 83.94817352294922\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 83.94817352294922\n",
      "Full Seq Acc: 0.2537023723125458\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.009012345224618912\n",
      "Abs Error: 1.4198681116104126\n",
      "Rel Error: 1.4504714012145996\n",
      "Smoothed Loss: 93.20737270073808\n",
      "Time: 0.6350641250610352\n",
      "\n",
      "Epoch: 82\n",
      "GPT loss: 86.49626922607422\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.49626922607422\n",
      "Full Seq Acc: 0.25458452105522156\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.007876923307776451\n",
      "Abs Error: 1.404750943183899\n",
      "Rel Error: 1.4352476596832275\n",
      "Smoothed Loss: 93.14026166599145\n",
      "Time: 0.6360440254211426\n",
      "\n",
      "Epoch: 83\n",
      "GPT loss: 86.09391784667969\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.09391784667969\n",
      "Full Seq Acc: 0.2548640966415405\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.010356306098401546\n",
      "Abs Error: 1.396882176399231\n",
      "Rel Error: 1.4274996519088745\n",
      "Smoothed Loss: 93.06979822779833\n",
      "Time: 0.6356360912322998\n",
      "\n",
      "Epoch: 84\n",
      "GPT loss: 84.0886001586914\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 84.0886001586914\n",
      "Full Seq Acc: 0.2544955015182495\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.007050092797726393\n",
      "Abs Error: 1.3937402963638306\n",
      "Rel Error: 1.424631118774414\n",
      "Smoothed Loss: 92.97998624710725\n",
      "Time: 0.6365351676940918\n",
      "\n",
      "Epoch: 85\n",
      "GPT loss: 86.85547637939453\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.85547637939453\n",
      "Full Seq Acc: 0.25788283348083496\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.006509456783533096\n",
      "Abs Error: 1.375108003616333\n",
      "Rel Error: 1.4058781862258911\n",
      "Smoothed Loss: 92.91874114843013\n",
      "Time: 0.6365311145782471\n",
      "\n",
      "Epoch: 86\n",
      "GPT loss: 85.35546112060547\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 85.35546112060547\n",
      "Full Seq Acc: 0.2582600712776184\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0078808031976223\n",
      "Abs Error: 1.3606683015823364\n",
      "Rel Error: 1.391458511352539\n",
      "Smoothed Loss: 92.84310834815187\n",
      "Time: 0.6345152854919434\n",
      "\n",
      "Epoch: 87\n",
      "GPT loss: 84.52269744873047\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 84.52269744873047\n",
      "Full Seq Acc: 0.2575335204601288\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.006799357011914253\n",
      "Abs Error: 1.3525066375732422\n",
      "Rel Error: 1.3834691047668457\n",
      "Smoothed Loss: 92.75990423915766\n",
      "Time: 0.6351256370544434\n",
      "\n",
      "Epoch: 88\n",
      "GPT loss: 86.79170989990234\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.79170989990234\n",
      "Full Seq Acc: 0.26071223616600037\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0069110207259655\n",
      "Abs Error: 1.337398648262024\n",
      "Rel Error: 1.368390679359436\n",
      "Smoothed Loss: 92.70022229576512\n",
      "Time: 0.6356706619262695\n",
      "\n",
      "Epoch: 89\n",
      "GPT loss: 86.76527404785156\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.76527404785156\n",
      "Full Seq Acc: 0.2614631950855255\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0077691455371677876\n",
      "Abs Error: 1.3315682411193848\n",
      "Rel Error: 1.3627984523773193\n",
      "Smoothed Loss: 92.64087281328598\n",
      "Time: 0.6350157260894775\n",
      "\n",
      "Epoch: 90\n",
      "GPT loss: 86.23343658447266\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 86.23343658447266\n",
      "Full Seq Acc: 0.26090577244758606\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.005805336404591799\n",
      "Abs Error: 1.322102665901184\n",
      "Rel Error: 1.3535126447677612\n",
      "Smoothed Loss: 92.57679845099786\n",
      "Time: 0.6352908611297607\n",
      "\n",
      "Epoch: 91\n",
      "GPT loss: 85.90383911132812\n",
      "Energy Reg: 0.0\n",
      "Total_loss: 85.90383911132812\n",
      "Full Seq Acc: 0.261738121509552\n",
      "Ans Region Acc: 0.0\n",
      "Ans Char Acc: 0.0070405141450464725\n",
      "Abs Error: 1.3122014999389648\n",
      "Rel Error: 1.343804955482483\n",
      "Smoothed Loss: 92.51006885760117\n",
      "Time: 0.6360182762145996\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m train_data = [x.to(device) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m train_data]\n\u001b[32m     15\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) == \u001b[32m1\u001b[39m:\n\u001b[32m     20\u001b[39m     data = result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(model, train_data, mean_len, optimizer, scheduler, scaler, args, calculate_errors)\u001b[39m\n\u001b[32m     43\u001b[39m     data = data + [\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m]\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     data = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m.cpu().numpy()\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data, safe_params\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "record = np.zeros((epochs, 9))\n",
    "num_NaNs = 0\n",
    "smoothed_loss = None\n",
    "\n",
    "safe_params = [copy.deepcopy(i.state_dict()) for i in [model, optimizer, scheduler]]\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "for train_data in dataloader:\n",
    "    if epoch >= epochs:\n",
    "        break\n",
    "\n",
    "    train_data = [x.to(device) for x in train_data]\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    result = train_step(model, train_data, mean_len, optimizer, scheduler, scaler, args, calculate_errors=not args.deterministic)\n",
    "\n",
    "    if len(result) == 1:\n",
    "        data = result\n",
    "        total_loss = data[0]\n",
    "        num_NaNs += 1\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(\"Instability detected\")\n",
    "        print(f\"Total Loss: {total_loss.item()}\\n\")\n",
    "        model.load_state_dict(safe_params[0])\n",
    "        optimizer.load_state_dict(safe_params[1])\n",
    "        scheduler.load_state_dict(safe_params[2])\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        continue\n",
    "\n",
    "    data, safe_params = result\n",
    "    smoothed_loss = 0.99 * smoothed_loss + 0.01 * data[2].item() if smoothed_loss is not None else data[2].item()\n",
    "    epoch = epoch + 1\n",
    "\n",
    "    record[epoch - 1, :-1] = data\n",
    "    record[epoch - 1, -1] = num_NaNs\n",
    "        \n",
    "    names = [\"GPT loss\", \"Energy Reg\", \"Total_loss\", \"Full Seq Acc\",\n",
    "             \"Ans Region Acc\", \"Ans Char Acc\", \"Abs Error\", \"Rel Error\"]\n",
    "\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for name, value in zip(names, data):\n",
    "        print(f\"{name}: {value}\")\n",
    "    print(f\"Smoothed Loss: {smoothed_loss}\")\n",
    "    print(f\"Time: {time.time() - t0}\\n\")\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        save_record(args.save_path, model, record, args, time_stamp)\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralode",
   "language": "python",
   "name": "neuralode"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
