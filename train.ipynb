{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"/accounts/grad/zhangyunzhe2023/stat 260/STAT-260-Final-Project-Randomized-Linear-Algebra-Transformer\")\n",
    "from RLALLaMA3.LLaMA3 import ModelArgs, Transformer\n",
    "from RLALLaMA3.utils import (\n",
    "    linear_warmup_cosine_decay_multiplicative,\n",
    "    name_args,\n",
    "    Args,\n",
    ")\n",
    "from RLALLaMA3.tasks import single_answer_seq_loss, get_dataset\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "time_stamp = time.strftime(\"%Y-%m-%d %H-%M-%S\", time.localtime())\n",
    "print(f\"Time stamp: {time_stamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the arguments\n",
    "##args = parse_args()\n",
    "dim = 256\n",
    "hidden_dim = 896\n",
    "n_heads = 4\n",
    "sample_ratio = 0.75\n",
    "projection_ratio = 0.5\n",
    "sv_sample_exact_dim = 48\n",
    "sv_projection_dim = 8\n",
    "\n",
    "head_dim = dim // n_heads\n",
    "\n",
    "args = Args(\n",
    "    # Training parameters\n",
    "    standard_lr=1e-3,\n",
    "    standard_epoch=100000,\n",
    "    standard_warmup_steps=2000,\n",
    "    batch_size=2048,\n",
    "    min_lr=1e-4,\n",
    "    grad_clip_max_norm=1.0,\n",
    "    use_amp=True,\n",
    "    use_compile=False,\n",
    "    model_type=\"transformer\",\n",
    "\n",
    "    # Data parameters\n",
    "    task=\"number_add\",\n",
    "    max_level=20,\n",
    "    random_seq_len=True,\n",
    "    number_range=(0, 99),\n",
    "\n",
    "    # Model architecture parameters\n",
    "    dim=dim,\n",
    "    n_layers=4,\n",
    "    n_heads=n_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "\n",
    "    # --- Randomized Linear Algebra (RLA) Parameters ---\n",
    "    deterministic=False,\n",
    "    sketch_mode='rademacher',\n",
    "\n",
    "    # RLA for Attention Linear Layers (Wq, Wk, Wv, Wo)\n",
    "    # dim\n",
    "    rla_attn_qkv_sample_exact_dim=round(dim * sample_ratio),\n",
    "    rla_attn_qkv_projection_dim=round((dim - round(dim * sample_ratio)) * projection_ratio),\n",
    "    # dim\n",
    "    rla_attn_out_sample_exact_dim=round(dim * sample_ratio),\n",
    "    rla_attn_out_projection_dim=round((dim - round(dim * sample_ratio)) * projection_ratio),\n",
    "\n",
    "    # RLA for Feed-Forward Network (FFN) Linear Layers\n",
    "    # dim\n",
    "    rla_ffn_in_sample_exact_dim=round(dim * sample_ratio),\n",
    "    rla_ffn_in_projection_dim=round((dim - round(dim * sample_ratio)) * projection_ratio),\n",
    "    # hidden_dim\n",
    "    rla_ffn_out_sample_exact_dim=round(hidden_dim * sample_ratio),\n",
    "    rla_ffn_out_projection_dim=round((hidden_dim - round(hidden_dim * sample_ratio)) * projection_ratio),\n",
    "\n",
    "    # RLA for Scaled Dot-Product Attention (SDPA) internal matmuls\n",
    "    # head_dim\n",
    "    rla_sdpa_qk_sample_exact_dim=round(head_dim * sample_ratio),\n",
    "    rla_sdpa_qk_projection_dim=round((head_dim - round(head_dim * sample_ratio)) * projection_ratio),\n",
    "    # seq_len\n",
    "    rla_sdpa_sv_sample_exact_dim=sv_sample_exact_dim,\n",
    "    rla_sdpa_sv_projection_dim=sv_projection_dim,\n",
    "\n",
    "    # Save path\n",
    "    save_path=\"ckpt\",\n",
    "    final_save_path=\"ckpt_final\",\n",
    ")\n",
    "\n",
    "\n",
    "print(args, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "dataset, collate_fn, vocab_size, max_seq_len = get_dataset(args.task,\n",
    "                                                           args.max_level,\n",
    "                                                           args.random_seq_len,\n",
    "                                                           args.number_range,\n",
    "                                                           nested_tensor=False,\n",
    "                                                           pad_to_longest=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, collate_fn=collate_fn,\n",
    "                                         num_workers=torch.get_num_threads(), pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_seq_len(dataloader, num_samples=100):\n",
    "    \"\"\"\n",
    "    Calculate the mean sequence length of the dataset.\n",
    "    \"\"\"\n",
    "    total_len = 0\n",
    "    num_samples = min(num_samples, len(dataloader.dataset))\n",
    "    for i, x in enumerate(dataloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        total_len += x[1].float().mean().item()\n",
    "    return total_len / num_samples\n",
    "\n",
    "mean_len = mean_seq_len(dataloader)\n",
    "print(f\"Mean sequence length: {mean_len}\")\n",
    "print(f\"Max sequence length: {max_seq_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model\n",
    "\n",
    "transformer_args = ModelArgs(\n",
    "    # Standard model parameters from Args\n",
    "    dim=args.dim,\n",
    "    n_layers=args.n_layers,\n",
    "    n_heads=args.n_heads,\n",
    "    hidden_dim=args.hidden_dim, # ModelArgs.__post_init__ might adjust this if None\n",
    "    vocab_size=vocab_size,      # Assumed to be defined in the scope\n",
    "    max_seq_len=max_seq_len,    # Assumed to be defined in the scope\n",
    "\n",
    "    # Parameters that might be hardcoded or could be added to Args if needed\n",
    "    norm_eps=1e-5,              # Default in ModelArgs, can be overridden\n",
    "    rope_theta=500000.0,        # Default in ModelArgs, can be overridden\n",
    "    # n_kv_heads=args.n_kv_heads, # If you add n_kv_heads to Args and ModelArgs\n",
    "\n",
    "    # RLA parameters from Args\n",
    "    deterministic=args.deterministic,\n",
    "    sketch_mode=args.sketch_mode,\n",
    "\n",
    "    # RLA for Attention Linear Layers\n",
    "    rla_attn_qkv_sample_exact_dim=args.rla_attn_qkv_sample_exact_dim,\n",
    "    rla_attn_qkv_projection_dim=args.rla_attn_qkv_projection_dim,\n",
    "    rla_attn_out_sample_exact_dim=args.rla_attn_out_sample_exact_dim,\n",
    "    rla_attn_out_projection_dim=args.rla_attn_out_projection_dim,\n",
    "\n",
    "    # RLA for Feed-Forward Network (FFN) Linear Layers\n",
    "    rla_ffn_in_sample_exact_dim=args.rla_ffn_in_sample_exact_dim,\n",
    "    rla_ffn_in_projection_dim=args.rla_ffn_in_projection_dim,\n",
    "    rla_ffn_out_sample_exact_dim=args.rla_ffn_out_sample_exact_dim,\n",
    "    rla_ffn_out_projection_dim=args.rla_ffn_out_projection_dim,\n",
    "\n",
    "    # RLA for Scaled Dot-Product Attention (SDPA) internal matmuls\n",
    "    rla_sdpa_qk_sample_exact_dim=args.rla_sdpa_qk_sample_exact_dim,\n",
    "    rla_sdpa_qk_projection_dim=args.rla_sdpa_qk_projection_dim,\n",
    "    rla_sdpa_sv_sample_exact_dim=args.rla_sdpa_sv_sample_exact_dim,\n",
    "    rla_sdpa_sv_projection_dim=args.rla_sdpa_sv_projection_dim,\n",
    ")\n",
    "\n",
    "model = Transformer(params=transformer_args)\n",
    "\n",
    "model = model.to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "standard_lr = args.standard_lr / 512\n",
    "standard_epoch = args.standard_epoch * 512\n",
    "standard_warmup_steps = args.standard_warmup_steps * 512\n",
    "batch_size = args.batch_size\n",
    "\n",
    "lr = standard_lr * batch_size\n",
    "warmup_steps = standard_warmup_steps // batch_size\n",
    "epochs = standard_epoch // batch_size\n",
    "\n",
    "print(\"Derived Parameters:\")\n",
    "print(f\"lr: {lr}\")\n",
    "print(f\"warmup_steps: {warmup_steps}\")\n",
    "print(f\"epochs: {epochs}\")\n",
    "print(f\"grad_clip_max_norm: {args.grad_clip_max_norm}\", end=\"\\n\\n\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, fused=True)\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer,\n",
    "            lr_lambda=lambda step: linear_warmup_cosine_decay_multiplicative(step, warmup_steps, epochs, args.min_lr))\n",
    "\n",
    "scaler = torch.amp.GradScaler(device, enabled=args.use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and arguments\n",
    "\n",
    "def save_record(path, model, record, args, time_stamp, extra_info=None):\n",
    "    dict_name, file_name = name_args(args, \"_\")\n",
    "\n",
    "    os.makedirs(f\"{path}/{dict_name}\", exist_ok=True)\n",
    "    file_name = file_name + f\"_{time_stamp}\"\n",
    "    if extra_info is not None:\n",
    "        file_name += f\"_{extra_info}\"\n",
    "    \n",
    "    record_dict = {\n",
    "        \"model\": model,\n",
    "        \"record\": record,\n",
    "        \"args\": args,\n",
    "        \"time_stamp\": time_stamp,\n",
    "    }\n",
    "        \n",
    "    torch.save(record_dict, f\"{path}/{dict_name}/{file_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backwards pass\n",
    "def backward_pass(model, loss, optimizer, scaler, scheduler, grad_clip_max_norm):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_max_norm)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_output_error(transformer_output, gt_transformer_output, pad_mask):\n",
    "    \"\"\"\n",
    "    Calculate the error between the transformer output and the ground truth output.\n",
    "    \"\"\"\n",
    "    # Apply the padding mask to both outputs\n",
    "    pad_mask = pad_mask.unsqueeze(-1)\n",
    "    transformer_output = transformer_output * pad_mask\n",
    "    gt_transformer_output = gt_transformer_output * pad_mask\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    abs_error = (transformer_output - gt_transformer_output).square().sum()\n",
    "    relative_error = abs_error / gt_transformer_output.square().sum()\n",
    "    num_tokens = pad_mask.sum()\n",
    "    abs_error = abs_error / num_tokens / transformer_output.size(-1)\n",
    "    return abs_error, relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(disable=not args.use_compile)\n",
    "def train_step(model, train_data, mean_len, optimizer, scheduler, scaler, args, calculate_errors=False):\n",
    "    device = train_data[0].device\n",
    "    \n",
    "    with torch.autocast(device_type=device.type, dtype=torch.bfloat16, enabled=args.use_amp):\n",
    "        tokens, lengths, ans_starts, ans_lengths = train_data\n",
    "\n",
    "        if calculate_errors:\n",
    "            with torch.no_grad():\n",
    "                pad_mask = tokens[:, 1:] == 0\n",
    "                model.deterministic_mode(True)\n",
    "                _, gt_transformer_output = model(tokens[:, :-1], return_transformer_output=True)\n",
    "                model.deterministic_mode(args.deterministic)\n",
    "\n",
    "        pred, transformer_output = model(tokens[:, :-1], return_transformer_output=True)\n",
    "        transformer_output = transformer_output.detach()\n",
    "\n",
    "        if calculate_errors:\n",
    "            with torch.no_grad():\n",
    "                abs_error, relative_error = transformer_output_error(transformer_output, gt_transformer_output, pad_mask)\n",
    "                abs_error, relative_error = abs_error.detach(), relative_error.detach()\n",
    "        \n",
    "        result = single_answer_seq_loss(pred, tokens, lengths, ans_starts, ans_lengths)\n",
    "        GPT_loss, full_seq_acc, ans_region_acc, ans_char_acc = result\n",
    "        # Normalize the GPT loss by the batch size but not the sequence length\n",
    "        GPT_loss = GPT_loss / args.batch_size\n",
    "        total_loss = GPT_loss\n",
    "        total_loss_for_backward = total_loss / mean_len\n",
    "    \n",
    "    if torch.isnan(total_loss) or torch.isinf(total_loss):# or (total_loss > smoothed_loss * 1.1):\n",
    "        return [total_loss]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        safe_params = [copy.deepcopy(i.state_dict()) for i in [model, optimizer, scheduler]]\n",
    "\n",
    "    backward_pass(model, total_loss_for_backward, optimizer, scaler, scheduler, args.grad_clip_max_norm)\n",
    "    \n",
    "    data = [GPT_loss, 0, total_loss, full_seq_acc, ans_region_acc, ans_char_acc]\n",
    "\n",
    "    if calculate_errors:\n",
    "        data = data + [abs_error, relative_error]\n",
    "    else:\n",
    "        data = data + [0, 0]\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        data = torch.tensor(data).cpu().numpy()\n",
    "\n",
    "    return data, safe_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = np.zeros((epochs, 9))\n",
    "num_NaNs = 0\n",
    "smoothed_loss = None\n",
    "\n",
    "safe_params = [copy.deepcopy(i.state_dict()) for i in [model, optimizer, scheduler]]\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "for train_data in dataloader:\n",
    "    if epoch >= epochs:\n",
    "        break\n",
    "\n",
    "    train_data = [x.to(device) for x in train_data]\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    result = train_step(model, train_data, mean_len, optimizer, scheduler, scaler, args, calculate_errors=not args.deterministic)\n",
    "\n",
    "    if len(result) == 1:\n",
    "        data = result\n",
    "        total_loss = data[0]\n",
    "        num_NaNs += 1\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(\"Instability detected\")\n",
    "        print(f\"Total Loss: {total_loss.item()}\\n\")\n",
    "        model.load_state_dict(safe_params[0])\n",
    "        optimizer.load_state_dict(safe_params[1])\n",
    "        scheduler.load_state_dict(safe_params[2])\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        continue\n",
    "\n",
    "    data, safe_params = result\n",
    "    smoothed_loss = 0.99 * smoothed_loss + 0.01 * data[2].item() if smoothed_loss is not None else data[2].item()\n",
    "    epoch = epoch + 1\n",
    "\n",
    "    record[epoch - 1, :-1] = data\n",
    "    record[epoch - 1, -1] = num_NaNs\n",
    "        \n",
    "    names = [\"GPT loss\", \"Energy Reg\", \"Total_loss\", \"Full Seq Acc\",\n",
    "             \"Ans Region Acc\", \"Ans Char Acc\", \"Abs Error\", \"Rel Error\"]\n",
    "\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for name, value in zip(names, data):\n",
    "        print(f\"{name}: {value}\")\n",
    "    print(f\"Smoothed Loss: {smoothed_loss}\")\n",
    "    print(f\"Time: {time.time() - t0}\\n\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        save_record(args.save_path, model, record, args, time_stamp)\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralode",
   "language": "python",
   "name": "neuralode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
